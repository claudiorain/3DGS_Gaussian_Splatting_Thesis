\chapter{Implementazione dei Componenti}
\label{chap:implementazione}

\section{Architettura del Sistema}

Il sistema proposto implementa un'architettura distribuita a microservizi progettata per gestire l'intero workflow del 3D Gaussian Splatting, dall'upload del contenuto video alla visualizzazione dei modelli 3D generati. La Figura \ref{fig:system_architecture} presenta una vista complessiva dell'architettura, evidenziando i componenti principali e i loro pattern di interazione.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/diagramma_architettura.jpg}
	\caption{Architettura complessiva del sistema distribuito}
	\label{fig:system_architecture}
\end{figure}

\subsection{Organizzazione a Layer}

L'architettura Ã¨ strutturata in layer distinti, ciascuno con responsabilitÃ  specifiche e interfacce ben definite, seguendo i principi di separazione delle responsabilitÃ  e scalabilitÃ  indipendente.

\subsubsection{Layer di Presentazione}

Il \textbf{layer di presentazione} costituisce l'interfaccia utente del sistema e comprende l'applicazione web sviluppata in Vue.js integrata con il viewer 3D GaussianSplats3D. Questo layer gestisce l'interazione utente, la visualizzazione dei modelli generati e la gestione degli stati dell'interfaccia. La scelta di Vue.js consente un'architettura component-based che facilita la manutenibilitÃ  e l'estensibilitÃ  dell'interfaccia.

\subsubsection{Layer API Gateway}

Il \textbf{layer API Gateway} funge da punto di ingresso unificato per tutte le richieste del sistema, implementando pattern di routing, autenticazione e gestione delle richieste. L'API Gateway, sviluppato in FastAPI, espone endpoint REST standardizzati e coordina l'accesso ai servizi sottostanti. Il servizio di upload gestisce specificamente il meccanismo di presigned URL per l'upload diretto su Amazon S3, ottimizzando il throughput e riducendo il carico sul backend.

\subsubsection{Layer di Orchestrazione}

Il \textbf{layer di orchestrazione} rappresenta il cuore coordinativo del sistema, implementato attraverso RabbitMQ come message broker e il Job Executor come coordinatore del workflow. Questo layer gestisce la comunicazione asincrona tra i componenti e implementa un pattern Producer-Consumer ibrido che garantisce l'esecuzione sequenziale delle fasi di elaborazione. La scelta di RabbitMQ assicura persistenza dei messaggi, gestione delle code specializzate e resilienza in caso di fallimenti temporanei.

\subsubsection{Layer di Processing}

Il \textbf{layer di processing} comprende i servizi specializzati responsabili dell'elaborazione computazionale: Video Processing per l'estrazione frame, Point Cloud Service per la ricostruzione geometrica tramite COLMAP, e i Training Services per l'esecuzione degli algoritmi di Gaussian Splatting.

\subsubsection{Layer di Storage}

Il \textbf{layer di storage} implementa una strategia ibrida che combina MongoDB per metadati e stato delle elaborazioni, Amazon S3 per contenuti multimediali e modelli 3D, e cache locale per dati di processing temporanei. Questa architettura ottimizza l'accesso ai dati in base ai pattern di utilizzo specifici di ogni tipo di informazione.

\subsection{Flusso Dati tra Componenti}

Il sistema implementa pattern di comunicazione differenziati in base alle caratteristiche delle interazioni tra componenti.

\subsubsection{Comunicazione Sincrona}

La \textbf{comunicazione sincrona} Ã¨ utilizzata per operazioni immediate come autenticazione, richiesta di presigned URL, e query sui metadati. Il frontend comunica direttamente con l'API Gateway attraverso chiamate REST, mentre l'API Gateway accede sincronamente a MongoDB per operazioni CRUD sui metadati dei modelli.

\subsubsection{Comunicazione Asincrona}
La \textbf{comunicazione asincrona} gestisce le operazioni long-running attraverso il sistema di code messaggi. L'API Gateway inserisce job nelle code specializzate, il Job Executor rimane in ascolto sui messaggi, li processa utilizzando l'handler corretto e, quando quest'ultimo termina, inserisce il messaggio nella coda legata all'handler successivo. Il sistema processa un job alla volta per garantire un utilizzo ottimale delle risorse GPU disponibili. Questo pattern garantisce resilienza e permette la gestione di elaborazioni che possono richiedere da minuti a ore.

\subsubsection{Upload Pattern}
Il \textbf{pattern di upload} implementa un flusso ottimizzato per l'upload iniziale dei video che elimina il "doppio upload" (frontendâ†’backendâ†’S3). Il frontend richiede un presigned URL all'API Gateway, che coordina con il servizio di upload la generazione dell'URL temporaneo per l'upload diretto ad Amazon S3. Questo approccio riduce significativamente i tempi di upload, il carico sul backend e l'utilizzo della banda, evitando che video di grandi dimensioni debbano transitare attraverso i server applicativi.

\subsection{Pattern di Interazione e Comunicazione}

\subsubsection{Producer-Consumer Ibrido}

Il sistema implementa un pattern Producer-Consumer specializzato dove il Job Executor assume alternativamente ruoli di consumer (elaborazione job corrente) e producer (inserimento job successivo). Questo pattern ibrido Ã¨ ottimizzato per workflow sequenziali e garantisce la continuitÃ  dell'elaborazione senza coordinamento esterno.

\subsubsection{Event-Driven Processing}

L'architettura segue principi event-driven dove il completamento di ogni fase genera automaticamente l'evento per la fase successiva. Questo approccio disaccoppia temporalmente le fasi di elaborazione e permette resilienza in caso di fallimenti parziali.

\subsection{Stack Tecnologico Implementato}

\subsubsection{Tecnologie Frontend}

Il frontend utilizza \textbf{Vue.js 3} come framework principale per lo sviluppo dell'interfaccia utente, scelto per il bilanciamento tra semplicitÃ  e funzionalitÃ . La visualizzazione 3D Ã¨ implementata attraverso \textbf{GaussianSplats3D}, una libreria specializzata basata su Three.js che supporta nativamente il rendering di primitive gaussiane.

\subsubsection{Tecnologie Backend}

Il backend Ã¨ implementato in \textbf{Python} con \textbf{FastAPI} come framework per le API REST, scelto per le performance superiori e la generazione automatica di documentazione OpenAPI. La gestione asincrona utilizza le capacitÃ  native di Python con asyncio.

\subsubsection{Infrastruttura e Persistenza}

L'infrastruttura utilizza \textbf{Docker e Docker Compose} per la containerizzazione e l'orchestrazione locale, \textbf{RabbitMQ} come message broker per la comunicazione asincrona, \textbf{MongoDB} come database NoSQL per metadati, e \textbf{Amazon S3} per lo storage oggetti scalabile.

\subsubsection{Processing Specializzato}

I servizi di processing integrano \textbf{COLMAP} per Structure from Motion, \textbf{PyTorch} con estensioni CUDA per i training services, e \textbf{Sharp Frames} per l'estrazione video. Ogni servizio Ã¨ ottimizzato per l'utilizzo specifico di risorse GPU attraverso configurazioni CUDA dedicate.

L'architettura complessiva garantisce scalabilitÃ , manutenibilitÃ  e performance ottimali per l'elaborazione di contenuti 3D, fornendo una base solida per l'implementazione dei componenti specializzati descritti nei capitoli successivi.


\section{Sistema di Coordinamento Workflow}

Il sistema di code messaggi rappresenta il layer di comunicazione asincrona che coordina l'esecuzione sequenziale delle fasi di elaborazione. L'implementazione si basa su RabbitMQ come message broker e implementa un pattern Producer-Consumer specializzato per gestire workflow di training complessi.

\subsection{Configurazione e Deployment RabbitMQ}

\subsubsection{Infrastruttura Containerizzata}

Il sistema utilizza l'immagine ufficiale RabbitMQ con interfaccia di gestione integrata, configurata per garantire persistenza e monitoraggio:

\begin{lstlisting}[language=docker-compose, caption=Configurazione Docker Compose per RabbitMQ]
	rabbitmq:
	image: "rabbitmq:3-management"
	container_name: rabbitmq
	hostname: 3dgs-job-queue
	ports:
	- "15672:15672"  # Interfaccia di gestione RabbitMQ
	- "5672:5672"    # Porta di comunicazione AMQP
	healthcheck:
	test: ["CMD", "rabbitmqctl", "status"]
	interval: 10s
	timeout: 5s
	retries: 5
	volumes:
	- rabbitmq_data:/var/lib/rabbitmq/mnesia/
	- ./rabbitmq/definitions.json:/etc/rabbitmq/definitions.json
	environment:
	- RABBITMQ_DEFAULT_USER=${RABBIT_MQ_DEFAULT_USER}
	- RABBITMQ_DEFAULT_PASS=${RABBIT_MQ_DEFAULT_PASS}
	command: >
		bash -c "rabbitmq-server & 
		until rabbitmqctl await_startup; do sleep 1; done; 
		rabbitmqctl import_definitions 	/etc/rabbitmq/definitions.json; 
		tail -f /dev/null"
\end{lstlisting}

\subsubsection{Architettura delle Code Specializzate}

Il sistema implementa cinque code dedicate, ciascuna responsabile di una fase specifica del workflow:

\begin{lstlisting}[language=python, caption=Definizione code specializzate per workflow]
	queues = [
	'frame_extraction_queue',      # Fase 1: Estrazione frame video
	'point_cloud_queue',           # Fase 2: Generazione nuvola punti  
	'model_training_queue',        # Fase 3: Training algoritmi GS
	'upload_queue',                # Fase 4: Upload modello 3D
	'metrics_generation_queue'     # Fase 5: Calcolo metriche qualitÃ 
	]
\end{lstlisting}

Ogni coda Ã¨ configurata con \texttt{durable=True} per garantire la persistenza dei messaggi anche in caso di riavvio del broker:

\begin{lstlisting}[language=python, caption=Inizializzazione code durevoli]
	def declare_queues(self):
	"""Dichiara le code se non esistono"""
	for queue_name in queues:
	self.channel.queue_declare(queue=queue_name, durable=True)
	print(f"Queue '{queue_name}' dichiarata.")
\end{lstlisting}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{images/rabbitmq_queues_interface.jpg}
	\caption{Interfaccia di gestione RabbitMQ - Vista code attive del sistema}
	\label{fig:rabbitmq_queues}
\end{figure}

\subsection{Pattern Producer-Consumer Ibrido}
Il sistema implementa una variante del pattern Producer-Consumer tradizionale, caratterizzata da un comportamento ibrido del Job Executor che si alterna tra ruoli di consumer e producer durante l'esecuzione del workflow sequenziale.

\subsubsection{API Gateway come Producer Iniziale}

Oltre ad rappresentare il punto di ingresso del sistema, l'API Gateway funge esclusivamente da producer, inserendo i job iniziali nelle code appropriate attraverso un mapping fase-coda standardizzato:

\begin{lstlisting}[language=python, caption=Implementazione Producer - API Gateway]
	from app.models.model import Phase
	
	# Mappa fase -> coda
	PHASE_TO_QUEUE = {
		Phase.FRAME_EXTRACTION: "frame_extraction_queue",
		Phase.POINT_CLOUD_BUILDING: "point_cloud_queue", 
		Phase.TRAINING: "model_training_queue",
		Phase.UPLOAD: "upload_queue",
		Phase.METRICS: "metrics_generation_queue"
	}
	
	class QueueJobService:
	def send_job(self, model_id: str, phase_str: str, additional_data=None):
	"""Invia un messaggio alla coda specificata"""
	try:
	phase = Phase(phase_str)
	except ValueError:
	raise ValueError(f"Invalid phase: {phase_str}")
	
	queue_name = PHASE_TO_QUEUE.get(phase)
	message = self.create_job_message(model_id, additional_data)
	
	self.channel.basic_publish(
	exchange='',
	routing_key=queue_name,
	body=message,
	properties=pika.BasicProperties(
	delivery_mode=2,  # Messaggio persistente
	)
	)
\end{lstlisting}

\subsubsection{Job Executor come Consumer-Producer ibrido}

Il Job Executor adotta un comportamento ibrido che si alterna dinamicamente tra consumer e producer. Agisce come \textbf{consumer} elaborando i messaggi dalle code ed eseguendo i job corrispondenti, quindi assume il ruolo di \textbf{producer} al termine del job corrente inserendo il messaggio nella coda successiva per far sÃ¬ che venga eseguito il job successivo (a meno che non si sia arrivati alla fine del workflow):

\begin{lstlisting}[language=python, caption=Implementazione Consumer sequenziale]
	async def process_queues_sequentially(self):
	"""Processa le code in sequenza"""
	
	while self.running:
		job_processed = False
	
		for queue_name in queues:
		# Controlla se ci sono messaggi nella coda corrente
			method_frame, header_frame, body = self.channel.basic_get(queue=queue_name, auto_ack=False)
	
			if method_frame:
				print(f" [*] Processing job from {queue_name}")
				# Processa il messaggio
				await self.process_job(self.channel, method_frame, header_frame, body)
				job_processed = True
				# Dopo aver processato un job, ricomincia dalla prima coda
				break
	
		# Se nessun job Ã¨ stato processato in questo ciclo, 	attendi un po' prima di ricontrollare
		if not job_processed:
			await asyncio.sleep(5)  # Pausa di 5 secondi prima di ricontrollare le code
\end{lstlisting}

\subsection{Gestione Job e Routing}

\subsubsection{Dispatcher Centralizzato con Transizione Consumer-Producer}

Il sistema utilizza un dispatcher centralizzato che effettua il routing dei messaggi agli handler specifici in base alla coda di origine:

\begin{lstlisting}[language=python, caption=Dispatcher con comportamento ibrido Consumer-Producer]
	async def process_job(self, ch, method, properties, body):
	"""Processa un singolo job"""
	try:
		data = json.loads(body.decode())
		model_id = data.get("model_id")
		print(f"Elaborazione del job: {data}")
	
		if not model_id:
			print("Errore: model_id mancante nel messaggio")
			ch.basic_ack(delivery_tag=method.delivery_tag)
			return
	
		# Conferma messaggio dopo parsing
		ch.basic_ack(delivery_tag=method.delivery_tag)
	
		# FASE CONSUMER: Routing ed esecuzione handler
		success = False
			if method.routing_key == "frame_extraction_queue":
			success = await self.job_service.handle_frame_extraction(ch, method,model_id, data)
			next_queue = "point_cloud_queue" if success else None
		
	elif method.routing_key == "point_cloud_queue":
		success = await self.job_service.handle_point_cloud_building(ch, method,model_id, data)
		next_queue = "model_training_queue" if success else None
	
	elif method.routing_key == "model_training_queue":
	success = await self.job_service.handle_training(ch, method,model_id, data)
	next_queue = "upload_queue" if success else None
	
	elif method.routing_key == "upload_queue":
	success = await self.job_service.handle_model_upload(ch, method,model_id, data)
	next_queue = "metrics_generation_queue" if success else None
	
	elif method.routing_key == "metrics_generation_queue":
	success = await self.job_service.handle_metrics_generation(ch, method,model_id, data)
	next_queue = None  # Fine workflow
	
	# FASE PRODUCER: Gestione centralizzata della coda successiva
	if success and next_queue:
	self.send_to_next_phase(model_id, next_queue)
	
	
	
	print(f"Job completato: {method.routing_key} per model_id: {model_id}")
	
	except Exception as e:
	print(f"âŒ Errore durante l'elaborazione del job: {e}")
	if 'model_id' in locals() and model_id:
	self.model_service.update_model_status(model_id, {"status": "ERROR", "error_message": str(e)})
	# Conferma comunque il messaggio per evitare di bloccarsi su messaggi problematici
	ch.basic_ack(delivery_tag=method.delivery_tag)
	print("Job fallito, conferma del messaggio alla coda.")
\end{lstlisting}

\subsubsection{Coordinamento Sequenziale tra Fasi}

Il metodo \texttt{send\_to\_next\_phase} rappresenta la transizione dal ruolo consumer al ruolo producer, garantendo la continuitÃ  del workflow:

\begin{lstlisting}[language=python, caption=Coordinamento sequenziale tra fasi di elaborazione]
	def send_to_next_phase(self, model_id, next_queue, additional_data=None):
	"""Invia il job alla fase successiva"""
	self.channel.basic_publish(
	exchange='',
	routing_key=next_queue,
	body=json.dumps({"model_id": model_id, "additional_data": additional_data})
	)
\end{lstlisting}

Questo approccio crea una catena di elaborazione dove ogni fase completata automaticamente innesca la successiva, eliminando la necessitÃ  di coordinamento esterno e garantendo un flusso continuo attraverso le cinque fasi del workflow (Frame Extraction â†’ Point Cloud â†’ Training â†’ Upload â†’ Metrics).

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/rabbitmq_message_example.jpg}
	\caption{Esempio di messaggio elaborato dal sistema (log job executor)}
	\label{fig:rabbitmq_message}
\end{figure}
\subsection{Sistema di Parametrizzazione Adattiva per QualitÃ  e Hardware}

Il workflow implementa un sistema di parametrizzazione multi-livello progettato per bilanciare qualitÃ  dell'output e vincoli hardware, provando a garantire sempre la generazione di un modello 3D indipendentemente dalle limitazioni del sistema.

\textbf{Livelli di QualitÃ  Utente}: L'utente puÃ² scegliere tra tre preset (Fast, Balanced, Quality) che influenzano parametri critici come risoluzione target, numero di frame estratti, iterazioni di training e soglie di densificazione. Questa scelta rappresenta l'\textbf{intento} di qualitÃ  desiderato, ma non Ã¨ vincolante per l'esecuzione.

\textbf{Hardware Adaptation}: Il \texttt{TrainingParamsService} interviene come arbitro finale, rilevando automaticamente la VRAM disponibile e applicando un \textbf{downscaling conservativo} quando necessario. La logica prende sempre il minimo tra la risoluzione richiesta dal livello di qualitÃ  e quella supportata dall'hardware, tentando di garantire che anche sistemi con 8GB di VRAM possano completare il workflow.

\textbf{Graceful Degradation}: Questa architettura mira a scongiurare fallimenti del processo, preferendo una riduzione automatica della qualitÃ  piuttosto che un errore. Il sistema Ã¨ progettato per essere deployabile anche in ambienti con risorse limitate, con l'obiettivo di produrre sempre un risultato, seppur potenzialmente a risoluzione ridotta rispetto alle aspettative iniziali dell'utente.

\subsubsection{Architettura dei Parametri}

La configurazione di ogni algoritmo Ã¨ organizzata in categorie distinte, ciascuna con uno scopo specifico nel workflow:

\begin{lstlisting}[language=JavaScript, caption=Struttura parametri di configurazione algoritmo]
	{
		"algorithm_name": "gaussian_splatting_original",
		"display_name": "3D Gaussian Splatting (Fixed Iterations)",
		
		// === VALORI BASE (livello "balanced") ===
		"base_params": {
			"iterations": 30000,
			"densify_grad_threshold": 0.0002,
			"densification_interval": 100,
			"densify_until_iter": 15000,
			"eval": true
		},
		
		// === QUALITY MULTIPLIERS: SOLO training parameters ===
		"quality_multipliers": {
			"fast": {
				"iterations": 0.8,
				"densify_grad_threshold": 1.0,
				"densify_from_iter": 0.8,
				"densify_until_iter": 0.8,
				"densification_interval": 0.8
			},
			"balanced": { /* moltiplicatori 1.0 */ },
			"quality": { /* moltiplicatori 1.2 */ }
		},
		
		// === PREPROCESSING PARAMS: Separati e chiari ===
		"preprocessing_params": {
			"fast": {
				"target_width": 1920,
				"target_height": 1080,
				"target_frames": 150
			},
			"balanced": {
				"target_width": 2560,
				"target_height": 1440,
				"target_frames": 200
			},
			"quality": {
				"target_width": 3840,
				"target_height": 2160,
				"target_frames": 250
			}
		},
		
		// === HARDWARE CONFIG ===
		"hardware_config": {
			"baseline_vram_gb": 24,
			"resolution_thresholds": [
			{ "vram_threshold": 24, "target_width": 3840, "target_height": 2160 },
			{ "vram_threshold": 16, "target_width": 1920, "target_height": 1080 },
			{ "vram_threshold": 8, "target_width": 1280, "target_height": 720 }
			],
			"scaling_formulas": {
				"densify_grad_threshold": {
					"formula": "max(1.6, 4.0 - (vram_factor * 2.2))",
					"min": 1.6, "max": 4.0
				}
			}
		}
	}
\end{lstlisting}

\textbf{base\_params}: Definiscono i valori di partenza dell'algoritmo, tipicamente corrispondenti ai parametri di default della implementazione originale. Rappresentano la base dalla quale vengono costruiti tutti e tre i livelli di qualitÃ  attraverso l'applicazione dei moltiplicatori. Questi parametri influenzano direttamente il comportamento degli algoritmi di training (numero di iterazioni, soglie di densificazione, intervalli di ottimizzazione).

\textbf{quality\_multipliers}: Moltiplicatori applicati esclusivamente ai \texttt{base\_params} per generare i parametri finali di training per ciascun livello di qualitÃ . Il livello "fast" riduce tipicamente le iterazioni e accelera il processo, mentre "quality" incrementa la precisione computazionale a scapito dei tempi di elaborazione.

\textbf{preprocessing\_params}: Parametri che influenzano esclusivamente la fase di preprocessing (estrazione frame, ridimensionamento video) e che non vengono passati agli algoritmi di training. Include risoluzione target e numero di frame da estrarre, parametri che impattano sulla qualitÃ  del dataset di input ma rimangono esterni alla logica di training.

\textbf{hardware\_config}: Definisce le soglie VRAM e le formule di scaling automatico. I \texttt{resolution\_thresholds} stabiliscono le risoluzioni massime supportate per ogni configurazione hardware, mentre le \texttt{scaling\_formulas} permettono un adattamento dinamico dei parametri di training in base alla memoria disponibile.

\subsubsection{Processo di Calcolo Parametri}

Il \texttt{TrainingParamsService} implementa una pipeline di calcolo che trasforma le configurazioni di base in parametri finali pronti per l'esecuzione:

\begin{lstlisting}[language=Python, caption=Pipeline di calcolo parametri nel TrainingParamsService]
	def generate_params(self, engine: Engine, quality_level: QualityLevel, 
	gpu_memory_gb: Optional[float] = None) -> GeneratedParams:
	
	# 1. Carica configurazione algoritmo
	config = self.get_config_by_engine(engine)
	
	# 2. Auto-rileva VRAM se necessario
	if gpu_memory_gb is None:
	gpu_memory_gb = self._detect_gpu_memory()
	
	# 3. Applica quality multipliers (solo training params)
	training_params, preprocessing_params = self._apply_quality_transforms(
	config.base_params.copy(), 
	config.quality_multipliers.get(quality_level, {}),
	config.preprocessing_params.get(quality_level, {})
	)
	
	# 4. Hardware scaling con resolution adaptation
	training_params, hw_multipliers, final_preprocessing = \
	self._apply_hardware_scaling(training_params, config.hardware_config, 
	gpu_memory_gb, preprocessing_params)
	
	# 5. Validazione e pulizia parametri finali
	self._validate_params(training_params, config.validation_rules)
	training_params = self._clean_params(training_params)
	
	return GeneratedParams(
	final_params=training_params,        # Per i motori di training
	preprocessing_params=final_preprocessing  # Per preprocessing
	)
\end{lstlisting}

La separazione netta tra parametri di training e preprocessing consente al sistema di adattare indipendentemente la qualitÃ  del dataset di input (risoluzione, numero di frame) e la precisione computazionale del training, ottimizzando l'utilizzo delle risorse hardware disponibili.

\subsubsection{ConfigurabilitÃ  e Sperimentazione}

Un aspetto fondamentale dell'architettura Ã¨ la \textbf{persistenza delle configurazioni in MongoDB}, che offre all'owner dell'applicazione la possibilitÃ  di modificare dinamicamente i parametri senza dover ricompilare o ridistribuire il software. Questa caratteristica Ã¨ particolarmente vantaggiosa per:

\textbf{Ottimizzazione Empirica}: L'owner puÃ² testare diverse combinazioni di parametri, moltiplicatori di qualitÃ  e soglie hardware per trovare il bilanciamento ottimale tra qualitÃ  dell'output e performance del sistema. Le modifiche alle configurazioni diventano immediatamente operative per tutti i nuovi job.

\textbf{Adattamento Hardware-Specifico}: Ogni deployment puÃ² essere fine-tuned per l'hardware specifico disponibile, modificando le \texttt{resolution\_thresholds} e le \texttt{scaling\_formulas} in base alle caratteristiche delle GPU installate e ai risultati dei test empirici.

\textbf{Evoluzione degli Algoritmi}: L'aggiunta di nuovi algoritmi o l'aggiornamento di quelli esistenti richiede solamente l'inserimento di un nuovo documento nella collection \texttt{training\_params}, mantenendo la retrocompatibilitÃ  con il codice esistente.

Questa flessibilitÃ  consente un approccio iterativo al miglioramento delle performance, dove l'esperienza operativa puÃ² guidare l'evoluzione delle configurazioni verso setup sempre piÃ¹ ottimizzati per il contesto di deployment specifico.  

\subsection{Handler Specializzati per Fase}

Il Job Executor delega l'elaborazione specifica di ogni fase a handler dedicati, implementati come funzioni specializzate che gestiscono sia l'esecuzione del processing che il coordinamento con il sistema di storage per supportare funzionalitÃ  avanzate di clonazione e retry.

\subsubsection{Architettura degli Handler}

Il sistema implementa cinque handler principali, ciascuno responsabile di una fase specifica del workflow:

\paragraph{Handler Frame Extraction}

L'handler di estrazione frame rappresenta la prima fase critica del workflow, responsabile della trasformazione del video di input in un dataset di immagini ottimizzato per la ricostruzione 3D. Il processo combina algoritmi di selezione intelligente con parametri adattivi basati sull'hardware disponibile e sul livello di qualitÃ  scelto dall'utente.

\subparagraph{Inizializzazione e Download Video}

Il processo inizia con la preparazione dell'ambiente di lavoro e il recupero del video da processare. Il sistema sfrutta un meccanismo di cache intelligente per evitare download ridondanti:

\begin{lstlisting}[language=python, caption=Inizializzazione e download video]
	model_service.start_phase(model_id, "frame_extraction")
	model = model_service.get_model_by_id(model_id)
	video_s3_key = model.video_s3_key
	
	model_dir = os.path.join(WORKING_DIR, f"{model_id}")
	os.makedirs(model_dir, exist_ok=True)
	
	# Usa percorso fisso per sfruttare la cache
	local_video_path = os.path.join(model_dir, 'input_video.mp4')
	
	# Il repository_service gestisce automaticamente la cache
	# Se in cache: copia dalla cache al percorso locale
	# Se non in cache: scarica da S3 e copia in cache
	repository_service.download(video_s3_key, local_video_path)
\end{lstlisting}

\subparagraph{Calcolo Parametri di Estrazione}

Il sistema recupera i parametri ottimali tramite il \texttt{TrainingParamsService}, ottenendo i \texttt{preprocessing\_params} che includono \texttt{target\_frames}, \texttt{target\_width} e \texttt{target\_height}. Questi valori sono giÃ  stati limitati dalla configurazione hardware e rappresentano gli input per il calcolo dei parametri finali di estrazione:

\begin{lstlisting}[language=python, caption=Calcolo parametri di estrazione]
	engine = model.training_config.get('engine')
	quality_level = model.training_config.get('quality_level')
	
	generated_params = training_params_service.generate_params(
	Engine(engine), QualityLevel(quality_level)
	)
	
	# Usa preprocessing_params invece di final_params
	target_frames = generated_params.preprocessing_params.get('target_frames', 200)
	target_width = generated_params.preprocessing_params.get('target_width', 1280)
	target_height = generated_params.preprocessing_params.get('target_height', 720)
	
	# Calcola parametri ottimizzati considerando orientamento video
	actual_width = frame_extractor.calculate_target_width(
	local_video_path, target_width, target_height
	)
	
	target_fps = frame_extractor.calculate_extraction_fps(
	local_video_path, target_frame_count=target_frames
	)
\end{lstlisting}

\subparagraph{Estrazione Frame con Sharp-Frames}

L'estrazione vera e propria utilizza il tool \texttt{sharp-frames} con algoritmi di selezione intelligente. Il sistema costruisce dinamicamente il comando in base ai parametri calcolati:

\begin{lstlisting}[language=python, caption=Estrazione frame con sharp-frames]
	with tempfile.TemporaryDirectory() as temp_sharp_output:
	cmd = [
	"sharp-frames",
	local_video_path,
	temp_sharp_output,
	"--selection-method", "best-n",
	"--min-buffer", "3",
	"--fps", str(target_fps)
	]
	
	# Aggiungi --width solo se necessario
	if actual_width is not None:
	cmd.extend(["--width", str(actual_width)])
	
	result = subprocess.run(cmd, capture_output=True, text=True, check=True)
	
	sharped_frames = sorted([
	f for f in os.listdir(temp_sharp_output)
	if f.lower().endswith(('.jpg', '.jpeg', '.png'))
	])
\end{lstlisting}



\subparagraph{Finalizzazione e Upload Risultati}

La fase si conclude con l'upload dei frame processati su S3 e l'aggiornamento dei metadati del modello per il tracking delle fasi successive:

\begin{lstlisting}[language=python, caption=Finalizzazione e upload risultati]
	# Upload cartella input (frames) su S3 come ZIP
	is_zip_uploaded = phase_zip_helper.create_phase_zip_and_upload(
	model_id, model_dir, POINT_CLOUD_BUILDING_PHASE_ZIP_NAME, ['input']
	)
	
	# Aggiorna metadati fase
	phase_metadata = {
		"frame_count": len(filtered_count),
		"video_size_mb": video_size / 1024 / 1024,
		"processing_params": {
			"fps": target_fps,
			"width": actual_width
		}
	}
	
	model_service.complete_phase(model_id, "frame_extraction", metadata=phase_metadata)
	
	# Cleanup file temporanei
	if os.path.exists(local_video_path):
	os.remove(local_video_path)
\end{lstlisting}

\paragraph{Handler Point Cloud Building}

L'handler di point cloud building rappresenta la seconda fase del workflow, responsabile della ricostruzione della nuvola di punti 3D a partire dal dataset di immagini ottimizzato. Il processo utilizza una pipeline COLMAP containerizzata per garantire riproducibilitÃ  e isolamento computazionale.

\subparagraph{Recupero Dataset e Validazione}

Il sistema recupera il dataset di frame dalla fase precedente, gestendo automaticamente diverse fonti (S3, cache locale, o riprocessamento per operazioni di clone/retry):

\begin{lstlisting}[language=python, caption=Recupero dataset e validazione]
	model_service.start_phase(model_id, "point_cloud_building")
	model = model_service.get_model_by_id(model_id)
	
	model_dir = os.path.join(WORKING_DIR, f"{model_id}")
	input_dir = os.path.join(model_dir, 'input')
	
	if os.path.exists(input_dir) and os.listdir(input_dir):
	print(f"âœ… Directory input giÃ  esistente per model_id {model_id}")
	else:
	# Recupera ZIP dalla fase precedente
	point_cloud_zip_s3_key = f"{S3_STAGING_PREFIX}/{model.parent_model_id}/{POINT_CLOUD_BUILDING_PHASE_ZIP_NAME}"
	
	success = phase_zip_helper.download_and_extract_phase_zip(
	point_cloud_zip_s3_key, model_dir
	)
	
	if not success or not os.path.exists(input_dir):
	self.fail(model_id, "point_cloud_building", 
	f"Failed to download/extract input dataset")
	return False
	
	# Conta frame di input per metadata
	frame_files = [f for f in os.listdir(input_dir) 
	if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
	input_frame_count = len(frame_files)
\end{lstlisting}

\subparagraph{Generazione Point Cloud via COLMAP}

La ricostruzione della nuvola di punti avviene tramite chiamata al servizio COLMAP containerizzato. Il sistema monitora i tempi di esecuzione per analisi delle performance e debugging:

\begin{lstlisting}[language=python, caption=Generazione point cloud via COLMAP API]
	sparse_dir = os.path.join(model_dir, 'sparse')
	
	print(f"ðŸ”„ Avvio generazione point cloud per model_id {model_id}...")
	colmap_start_time = datetime.utcnow()
	
	# COLMAP API call
	convert_request = {"input_dir": model_dir}
	response = requests.post(COLMAP_API_URL + "/convert", json=convert_request)
	
	# Monitoraggio timing
	colmap_end_time = datetime.utcnow()
	colmap_duration = colmap_end_time - colmap_start_time
	colmap_duration_seconds = colmap_duration.total_seconds()
	
	if response.status_code != 200:
	self.fail(model_id, "point_cloud_building", f"Colmap error: {response.text}")
	return False
	
	# Verifica output COLMAP
	if not os.path.exists(sparse_dir) or not os.listdir(sparse_dir):
	self.fail(model_id, "point_cloud_building", 
	f"Error: no cloud point created for model_id {model_id}")
	return False
\end{lstlisting}

\subparagraph{Packaging e Persistenza Risultati}

La fase si conclude con il packaging della nuvola di punti generata, mantenendo sia i dati sparsi (point cloud) che le immagini processate per la fase di training successiva. I risultati vengono persistiti su S3 come fase intermedia recuperabile per operazioni di retry o clone:

\begin{lstlisting}[language=python, caption=Packaging e upload risultati]
	# Upload nuvola di punti e immagini per fase training
	is_zip_uploaded = phase_zip_helper.create_phase_zip_and_upload(
	model_id, model_dir, TRAINING_PHASE_ZIP_NAME, ['sparse', 'images']
	)
	
	if not is_zip_uploaded:
	self.fail(model_id, "training", 
	f"Error: Failed to create training phase ZIP for model_id {model_id}")
	return False
	
	# Aggiorna metadati fase con statistiche performance
	phase_metadata = {
		"input_frame_count": input_frame_count,
		"colmap_duration_seconds": round(colmap_duration_seconds, 2),
		"colmap_start_time": colmap_start_time.isoformat(),
		"colmap_end_time": colmap_end_time.isoformat(),
	}
	
	model_service.complete_phase(model_id, "point_cloud_building", metadata=phase_metadata)
\end{lstlisting}

Il design di questa fase privilegia la robustezza e la recuperabilitÃ : la nuvola di punti generata viene salvata sia localmente per l'immediata elaborazione successiva, sia su S3 come checkpoint intermedio. Questo approccio consente operazioni di retry efficienti in caso di fallimenti nelle fasi successive e supporta la funzionalitÃ  di clonazione modelli senza dover riprocessare l'intera pipeline COLMAP.

\paragraph{Handler Training}

L'handler di training rappresenta la fase computazionalmente piÃ¹ intensiva del workflow, responsabile dell'addestramento degli algoritmi Gaussian Splatting con configurazione dinamica basata sull'engine selezionato e sui parametri ottimizzati per l'hardware disponibile.

\subparagraph{Recupero Dataset di Training}

Il sistema recupera la nuvola di punti e le immagini processate dalla fase precedente, gestendo automaticamente il restore da S3 per supportare operazioni di retry e clone:

\begin{lstlisting}[language=python, caption=Recupero dataset di training]
	model_service.start_phase(model_id, "training")
	model = model_service.get_model_by_id(model_id)
	
	model_dir = os.path.join(WORKING_DIR, f"{model_id}")
	image_dir = os.path.join(model_dir, "images")
	sparse_dir = os.path.join(model_dir, "sparse")
	
	if os.path.exists(image_dir) and os.listdir(image_dir) and \
	os.path.exists(sparse_dir) and os.listdir(sparse_dir):
	print(f"âœ… Directories images e sparse giÃ  esistenti per model_id {model_id}")
	else:
	# Recupera ZIP dalla fase point cloud building
	training_zip_s3_key = f"{S3_STAGING_PREFIX}/{model.parent_model_id}/{TRAINING_PHASE_ZIP_NAME}"
	
	success = phase_zip_helper.download_and_extract_phase_zip(
	training_zip_s3_key, model_dir
	)
	
	if not success or not os.path.exists(image_dir) or not os.path.exists(sparse_dir):
	self.fail(model_id, "training", 
	f"Failed to download/extract training dataset")
	return False
\end{lstlisting}

\subparagraph{Configurazione Dinamica Parametri}

Il sistema genera i parametri di training ottimizzati utilizzando il \texttt{TrainingParamsService}, che applica i moltiplicatori di qualitÃ  e le limitazioni hardware per produrre la configurazione finale da passare all'engine selezionato:

\begin{lstlisting}[language=python, caption=Generazione parametri di training]
	engine = model.training_config.get('engine')
	quality_level = model.training_config.get('quality_level')
	
	if not engine:
	self.fail(model_id, "training", f"Error: No engine found in model {model_id}")
	return False
	
	# Genera parametri ottimizzati per engine e hardware
	generated_params = training_params_service.generate_params(
	Engine(engine), QualityLevel(quality_level)
	)
	
	# Prepara directory di output
	train_output_folder = os.path.join(model_dir, 'output')
	os.makedirs(train_output_folder, exist_ok=True)
	
	print(f"ðŸŽ¯ Training parameters: {generated_params.final_params}")
\end{lstlisting}

\subparagraph{Selezione Engine e Esecuzione Training}

Il sistema seleziona dinamicamente l'API dell'engine containerizzato corrispondente alla scelta dell'utente e avvia il processo di training con monitoring dei tempi di esecuzione:

\begin{lstlisting}[language=python, caption=Esecuzione training con engine selezionato]
	# Selezione API engine dalla mappatura
	api_url = engine_map.get(engine, {}).get('api-url')
	if not api_url:
	self.fail(model_id, "training", f"Error: No api url found for engine {engine}")
	return False
	
	# Costruzione richiesta di training
	train_request = {
		"input_dir": model_dir,
		"output_dir": train_output_folder,
		"params": generated_params.final_params,  # Parametri ottimizzati
	}
	
	print(f"ðŸŽ¯ Starting training with engine: {engine}")
	training_start_time = datetime.utcnow()
	
	# Chiamata API training containerizzato
	response = requests.post(f"{api_url}/train", json=train_request)
	
	training_end_time = datetime.utcnow()
	training_duration = training_end_time - training_start_time
	training_duration_seconds = training_duration.total_seconds()
	
	if response.status_code != 200:
	self.fail(model_id, "training", 
	f"Error: Training failed status code {response.status_code}")
	return False
	
	print(f"âœ… Training completato con successo")
\end{lstlisting}

\subparagraph{Finalizzazione e Metadati}

La fase si conclude con l'aggiornamento dei metadati del modello, includendo statistiche di performance e configurazioni utilizzate per future analisi e debugging:

\begin{lstlisting}[language=python, caption=Finalizzazione e metadati training]
	# Aggiorna metadati con statistiche performance
	phase_metadata = {
		"training_duration_seconds": round(training_duration_seconds, 2),
		"training_start_time": training_start_time.isoformat(),
		"training_end_time": training_end_time.isoformat(),
		"training_parameters": {
			"engine": engine,
			"quality_level": quality_level,
			"final_params": generated_params.final_params
		}
	}
	
	model_service.complete_phase(model_id, "training", metadata=phase_metadata)
\end{lstlisting}

La fase di training rappresenta il cuore computazionale del sistema, dove la flessibilitÃ  della configurazione parametrica incontra la potenza degli engine specializzati. Il design containerizzato garantisce isolamento e riproducibilitÃ , mentre il sistema di parametrizzazione dinamica assicura che ogni training sia ottimizzato per l'hardware disponibile e la qualitÃ  richiesta dall'utente.

\paragraph{Handler Upload}

L'handler di upload rappresenta la penultima fase del workflow, responsabile della conversione del modello addestrato in formato ottimizzato per la distribuzione web e del caricamento sicuro su S3. La decisione di separare questa fase dal training Ã¨ strategica: evita che problemi di rete o disservizi S3 costringano l'utente a rieffettuare il training, che puÃ² durare da diversi minuti a ore. Questa fase Ã¨ considerata secondaria e non puÃ² essere scelta come base di partenza per operazioni di clone, in quanto rappresenta un'elaborazione dell'output di training piuttosto che una fase di processamento primaria.

\subparagraph{Isolamento Strategico e Preparazione Output}

La separazione dell'upload dal training protegge l'investimento computazionale giÃ  sostenuto, permettendo retry della sola fase di upload senza perdere il lavoro di training completato:

\begin{lstlisting}[language=python, caption=Preparazione output e isolamento fase]
	model_service.start_phase(model_id, "upload")
	model = model_service.get_model_by_id(model_id)
	
	with tempfile.TemporaryDirectory() as temp_dir:
	model_dir = os.path.join(WORKING_DIR, f"{model_id}")
	
	# Verifica output di training
	output_dir = os.path.join(model_dir, 'output')
	if not os.path.exists(output_dir):
	self.fail(model_id, "upload", f"No folder output found")
	return False
	
	# Localizza file PLY dell'iterazione finale
	ply_path = job_utils.find_latest_iteration_folder(output_dir)
	cameras_file_path = os.path.join(output_dir, "cameras.json")
	
	engine = model.training_config.get('engine')
\end{lstlisting}

\subparagraph{Conversione PLYâ†’SPLAT per Ottimizzazione Web}

Il sistema converte il formato PLY (output standard dei tre algoritmi di training) in formato SPLAT, significativamente piÃ¹ leggero e ottimizzato per la fruibilitÃ  web. Per l'algoritmo Taming Ã¨ necessaria una conversione specifica dell'opacitÃ  delle gaussiane:

\begin{lstlisting}[language=python, caption=Conversione formato e ottimizzazione web]
	# Usa cartella temporanea per il file .splat ottimizzato
	gsplat_path = os.path.join(temp_dir, "point_cloud.splat")
	
	# Conversione PLYâ†’SPLAT con gestione opacitÃ  engine-specific
	save_splat_file(
	process_ply_to_splat(
	ply_path, 
	convert_taming_opacity=(engine == Engine.TAMING.value)
	), 
	gsplat_path
	)
	
	# Aggiungi file cameras.json per rendering
	shutil.copy(cameras_file_path, temp_dir)
\end{lstlisting}

La funzione \texttt{process\_ply\_to\_splat} implementa una logica di conversione sofisticata che gestisce le differenze tra algoritmi:

\begin{lstlisting}[language=python, caption=Logica conversione opacitÃ  per engine Taming]
	def process_ply_to_splat(ply_file_path, convert_taming_opacity=False):
	plydata = PlyData.read(ply_file_path)
	vert = plydata["vertex"]
	
	# Gestione opacitÃ  specifica per Taming
	if convert_taming_opacity:
	print("ðŸ”„ Applying Tamingâ†’Three.js opacity conversion...")
	
	original_opacities = np.array([v["opacity"] for v in vert])
	
	# Applica inverse sigmoid (logit) per compatibilitÃ  Three.js
	epsilon = 1e-7
	clamped = np.clip(original_opacities, epsilon, 1.0 - epsilon)
	logit_opacities = np.log(clamped / (1.0 - clamped))
	
	print(f"Original opacity range: [{original_opacities.min():.3f}, {original_opacities.max():.3f}]")
	print(f"Converted opacity range: [{logit_opacities.min():.3f}, {logit_opacities.max():.3f}]")
	
	# Ordinamento gaussiane per volume con opacitÃ  corretta
	if convert_taming_opacity:
	sorted_indices = np.argsort(
	-np.exp(vert["scale_0"] + vert["scale_1"] + vert["scale_2"])
	/ (1 + np.exp(-logit_opacities))
	)
	else:
	sorted_indices = np.argsort(
	-np.exp(vert["scale_0"] + vert["scale_1"] + vert["scale_2"])
	/ (1 + np.exp(-vert["opacity"]))
	)
	
	# Costruzione buffer SPLAT ottimizzato
	buffer = BytesIO()
	for idx in sorted_indices:
	v = plydata["vertex"][idx]
	
	# Calcolo alpha con opacitÃ  engine-specific
	if convert_taming_opacity:
	alpha = 1 / (1 + np.exp(-logit_opacities[idx]))
	else:
	alpha = 1 / (1 + np.exp(-v["opacity"]))
	
	# Scrittura dati gaussiana nel formato SPLAT
	# [position, scales, color+alpha, rotation quaternion]
	
	return buffer.getvalue()
\end{lstlisting}

\subparagraph{Packaging e Upload Finale}

Il sistema crea un package ZIP ottimizzato contenente il modello SPLAT e i metadati di camera, garantendo atomicitÃ  dell'upload e facilitÃ  di distribuzione:

\begin{lstlisting}[language=python, caption=Packaging e upload su S3]
	# Creazione ZIP con modello e metadati camera
	zip_filename = os.path.join(model_dir, "3d_model.zip")
	shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', temp_dir)
	
	# Upload atomico su S3 delivery
	print(f"Inizio del caricamento del modello {model_id} su S3...")
	zip_model_suffix = f"3d_model.zip"
	zip_model_s3_key = f"{S3_DELIVERY_PREFIX}/{model_id}/{zip_model_suffix}"
	
	try:
	repository_service.upload(zip_filename, zip_model_s3_key)
	print(f"âœ… Modello {model_id} caricato su S3: {zip_model_s3_key}")
	except Exception as e:
	self.fail(model_id, "upload", f"Errore durante l'upload su S3: {e}")
	return False
	
	# Finalizzazione fase con aggiornamento metadati
	model_service.complete_phase(model_id, "upload")
	model_service.update_model_status(model_id, {"zip_model_suffix": zip_model_suffix})
\end{lstlisting}

La conversione PLYâ†’SPLAT non solo riduce significativamente le dimensioni del file, ma risolve anche le incompatibilitÃ  tra le diverse implementazioni degli algoritmi di training e i viewer web. La gestione engine-specific dell'opacitÃ  garantisce che tutti i modelli, indipendentemente dall'algoritmo utilizzato, vengano renderizzati correttamente dal viewer sul frontend.

\paragraph{Handler Metrics Generation}

L'handler di metrics generation rappresenta la fase finale del workflow, responsabile della valutazione quantitativa del modello addestrato attraverso metriche standard di qualitÃ  visiva. Come per l'upload, questa fase poteva essere annessa all'handler del training, ma la scelta di isolarla Ã¨ strategica per molteplici ragioni: oltre ai benefici di robustezza giÃ  discussi per l'handler di upload, il processo di generazione metriche non Ã¨ a costo computazionale zero e puÃ² impiegare diversi minuti. L'isolamento permette al modello di essere giÃ  disponibile per il rendering nel frontend mentre le metriche vengono calcolate e visualizzate in background.

\subparagraph{Inizializzazione e Validazione Output}

Il sistema verifica la disponibilitÃ  dell'output di training e prepara l'ambiente per la generazione delle metriche di valutazione:

\begin{lstlisting}[language=python, caption=Inizializzazione e validazione output training]
	model_service.start_phase(model_id, "metrics_evaluation")
	model = model_service.get_model_by_id(model_id)
	
	model_dir = os.path.join(WORKING_DIR, f"{model_id}")
	
	# Verifica esistenza output del training
	output_dir = os.path.join(model_dir, 'output')
	if not os.path.exists(output_dir):
	self.fail(model_id, "metrics_evaluation", f"No folder output found")
	return False
	
	engine = model.training_config.get('engine', 'INRIA')
\end{lstlisting}

\subparagraph{Generazione Render e Calcolo Metriche}

Il sistema utilizza lo stesso engine containerizzato del training per generare render di valutazione e calcolare le metriche standard di qualitÃ  visiva (SSIM, PSNR, LPIPS):

\begin{lstlisting}[language=python, caption=Generazione render e calcolo metriche]
	engine = model.training_config.get('engine', 'INRIA')
	
	# Generazione render per valutazione
	render_request = {"output_dir": output_dir}
	response = requests.post(engine_map.get(engine).get('api-url') + "/render", json=render_request)
	response.raise_for_status()
	
	# Calcolo metriche di qualitÃ 
	metrics_request = {"output_dir": output_dir}
	response = requests.post(engine_map.get(engine).get('api-url') + "/metrics", json=metrics_request)
	response.raise_for_status()
	
	# Verifica generazione del file risultati
	results_json_path = os.path.join(output_dir, "results.json")
	if not os.path.exists(results_json_path):
	raise FileNotFoundError("Il file 'results.json' non Ã¨ stato trovato.")
	
	with open(results_json_path, 'r') as f:
	results_data = json.load(f)
\end{lstlisting}

\subparagraph{Parsing Intelligente delle Metriche}

Il sistema implementa una logica sofisticata per estrarre le metriche dal file JSON generato, gestendo le variazioni nella struttura dei dati in base al numero di iterazioni di training configurate nei parametri:

\begin{lstlisting}[language=python, caption=Estrazione metriche con gestione iterazioni]
	# Cerca chiavi che iniziano con "ours_" (includono numero iterazioni)
	ours_keys = [key for key in results_data.keys() if key.startswith("ours_")]
	results = None
	
	if len(ours_keys) == 1:
	# Una sola chiave "ours_", usala direttamente
	key = ours_keys[0]
	print(f"âœ… Found single ours key: {key}")
	results = job_utils._extract_metrics_from_section(results_data[key])
	
	elif len(ours_keys) > 1:
	# Multiple chiavi, seleziona quella con numero iterazioni piÃ¹ alto
	def extract_number(key):
	try:
	return int(key.split("_")[1])  # Estrae numero da "ours_30000"
	except (IndexError, ValueError):
	return 0
	
	latest_key = max(ours_keys, key=extract_number)
	print(f"âœ… Found multiple ours keys, using latest: {latest_key}")
	results = job_utils._extract_metrics_from_section(results_data[latest_key])
	
	else:
	# Fallback: cerca qualsiasi sezione con metriche standard
	for key, value in results_data.items():
	if isinstance(value, dict) and any(metric in value for metric in ["SSIM", "PSNR", "LPIPS"]):
	print(f"âœ… Found metrics in fallback key: {key}")
	results = job_utils._extract_metrics_from_section(value)
	break
	
	if not results:
	raise KeyError(f"Nessuna sezione con metriche trovata. Chiavi disponibili: {list(results_data.keys())}")
\end{lstlisting}

\subparagraph{Finalizzazione e Persistenza Metriche}

La fase si conclude con la persistenza delle metriche calcolate nei metadati del modello, rendendo immediatamente disponibili per visualizzazione nel frontend:

\begin{lstlisting}[language=python, caption=Persistenza metriche e completamento workflow]
	# Salvataggio metriche nei metadati del modello
	model_service.complete_phase(
	model_id, 
	"metrics_evaluation",
	overall_status="COMPLETED",
	metadata={"metrics": results}
	)
	
	print(f"âœ… Metriche generate e salvate per model {model_id}")
\end{lstlisting}

La gestione intelligente delle chiavi del file JSON Ã¨ fondamentale perchÃ© il nome delle sezioni include il numero di iterazioni specificato nella parametrizzazione (es. \texttt{ours\_30000} per 30.000 iterazioni). Questo collegamento diretto tra parametri di training e struttura delle metriche garantisce che la valutazione sia sempre coerente con la configurazione utilizzata per l'addestramento. L'isolamento di questa fase permette all'utente di visualizzare il modello 3D immediatamente dopo l'upload, mentre le metriche vengono calcolate in background e aggiornate progressivamente nell'interfaccia.

\subsubsection{Pattern di Gestione Dati Intermedi}

Le prime tre fasi del workflow (Frame Extraction, Point Cloud Reconstruction, Training) implementano un pattern specializzato per la gestione dei dati intermedi che abilita funzionalitÃ  avanzate di clonazione e retry.

\textbf{\subsubsection{Pattern di Denominazione per Archivi Intermedi}
}
Ogni fase salva i propri risultati in archivi compressi standardizzati su Amazon S3, seguendo la convenzione di denominazione \texttt{\{nome\_fase\}\_phase.zip}. Questa struttura permette il recupero selettivo di dati intermedi per operazioni di reprocessing.

\begin{lstlisting}[language=python, caption=Pattern recupero e salvataggio dati intermedi]
	# Recupero dati fase precedente
	success = phase_zip_helper.download_and_extract_phase_zip(
	phase_zip_s3_key, model_dir
	)
	
	# Salvataggio risultati fase corrente
	is_zip_uploaded = phase_zip_helper.create_phase_zip_and_upload(
	model_id, model_dir, PHASE_ZIP_NAME, ['directory_output']
	)
\end{lstlisting}

\subsubsection{FunzionalitÃ  di Fork e Retry}

Il pattern dei dati intermedi abilita la \textbf{fork e il retry di modelli} a partire da fasi intermedie, permettendo comparazioni algoritmiche e sperimentazione senza dover ripetere l'intero workflow.

\noindent \textbf{Scenario di utilizzo}: Un utente puÃ² biforcare un modello completato fino alla fase di Point Cloud Reconstruction per testare un algoritmo di training diverso, riutilizzando i risultati COLMAP giÃ  elaborati. Questo approccio riduce significativamente i tempi di sperimentazione e ottimizza l'utilizzo delle risorse computazionali.

\subsubsection{Meccanismo di Retry}

Il sistema supporta \textbf{retry intelligente} per modelli falliti, permettendo la ripresa dell'elaborazione dall'ultima fase completata con successo. Questo meccanismo Ã¨ particolarmente importante per gestire fallimenti temporanei di rete, indisponibilitÃ  di servizi esterni, o errori di configurazione che non compromettono i dati intermedi.

\subsubsection{Gestione Fasi Secondarie}

Le ultime due fasi (Upload e Metrics Generation) seguono un pattern semplificato che non genera dati intermedi persistenti. Queste fasi sono considerate \textbf{secondarie} rispetto al core workflow e sono progettate per essere idempotenti e facilmente rieseguibili.

\textbf{Upload Modello Finale}: Gestisce la preparazione e il caricamento del modello 3D per la visualizzazione, convertendo i risultati del training in formati ottimizzati per il web viewer.

\textbf{Generazione Metriche}: Calcola le metriche di qualitÃ  (PSNR, SSIM, LPIPS) confrontando rendering di test con immagini ground truth, salvando i risultati direttamente nel database.

La separazione di queste fasi permette la visualizzazione immediata dei modelli 3D generati mentre le metriche vengono calcolate in background, migliorando l'esperienza utente senza compromettere l'affidabilitÃ  del sistema.

\subsubsection{Integrazione con Storage Layer}

Gli handler implementano una strategia di storage ibrida che ottimizza l'accesso ai dati in base alle caratteristiche di ogni fase:

\begin{itemize}
	\item \textbf{Dati di input}: Recuperati da Amazon S3 all'inizio di ogni fase
	\item \textbf{Processing locale}: Elaborazione su storage locale per massimizzare le performance I/O
	\item \textbf{Risultati intermedi}: Persistenza su S3 per durabilitÃ  e accessibilitÃ  cross-fase
	\item \textbf{Metadati}: Aggiornamento real-time su MongoDB per tracking dello stato
\end{itemize}

\subsection{Resilienza e Fault Tolerance}

\subsubsection{Gestione Errori e Recovery}

Il sistema implementa una strategia di gestione errori che privilegia la continuitÃ  operativa:

\begin{lstlisting}[language=python, caption=Gestione centralizzata degli errori]
	def fail(self, model_id: str, phase_str: str, error_message: str):
	"""Gestisce il fallimento di una fase"""
	print(error_message)
	model_service.fail_phase(model_id, phase_str, error_message)
	model_service.update_model_status(model_id, {"overall_status": "FAILED"})
\end{lstlisting}

\subsubsection{Caratteristiche di Resilienza}

Il sistema garantisce robustezza attraverso diverse strategie:

\begin{itemize}
	\item \textbf{Acknowledgment esplicito}: I messaggi vengono confermati solo dopo l'elaborazione completa
	\item \textbf{Error isolation}: Gli errori in un job non impattano l'elaborazione di altri job  
	\item \textbf{Persistent messaging}: I messaggi sopravvivono ai riavvii del sistema
	\item \textbf{Graceful shutdown}: Gestione pulita della chiusura attraverso signal handler
\end{itemize}

\subsubsection{Monitoraggio e OsservabilitÃ }

L'interfaccia di gestione RabbitMQ fornisce visibilitÃ  completa sullo stato del sistema:

\begin{itemize}
	\item \textbf{Code monitoring}: Numero di messaggi in attesa, in elaborazione e completati
	\item \textbf{Connection tracking}: Monitoraggio delle connessioni attive tra producer e consumer
	\item \textbf{Performance metrics}: Throughput, latenza e tasso di errore per ogni coda
\end{itemize}

Il sistema mantiene connessioni dedicate per api-gateway (producer) e job-executor (consumer), garantendo isolamento e scalabilitÃ  indipendente dei componenti.

\subsection{Video Preprocessing Service}

\subsubsection{Architettura Integrata nel Job Executor}
A differenza degli altri servizi computazionali (COLMAP e algoritmi di training) che operano in container Docker dedicati, il Video Preprocessing Service Ã¨ integrato direttamente nel container del Job Executor. Questa scelta architetturale Ã¨ motivata da diverse considerazioni tecniche e operative:

\begin{itemize}
	\item \textbf{Efficienza I/O}: Evita il trasferimento di video di grandi dimensioni tra container, riducendo latenza e utilizzo della rete
	\item \textbf{Gestione della cache}: Sfrutta il sistema di cache locale per video giÃ  processati, ottimizzando le operazioni di retry e fork
	\item \textbf{Controllo del flusso}: Permette una gestione diretta delle risorse durante l'estrazione, facilitando il monitoraggio e la gestione degli errori
	\item \textbf{SemplicitÃ  deployment}: Riduce la complessitÃ  dell'orchestrazione container eliminando un servizio esterno
\end{itemize}

\subsubsection{Pipeline di Preprocessing}
Il processo di preprocessing video implementa una pipeline ottimizzata in quattro fasi principali:

\begin{enumerate}
	\item \textbf{Calcolo Parametri Ottimali}: Analisi delle caratteristiche video e calcolo di parametri di estrazione adattivi
	\item \textbf{Estrazione Frame con Sharp-Frames}: Selezione intelligente di frame basata su metriche di qualitÃ 
	\item \textbf{Gestione Output}: Organizzazione e upload dei risultati per le fasi successive
	\item \textbf{Generazione Metadati}: Creazione di informazioni di tracciabilitÃ  per debug e ottimizzazione
\end{enumerate}

\begin{lstlisting}[language=python, caption=Struttura handler frame extraction]
	def handle_frame_extraction(self, ch, method, model_id, data):
	# Download video con gestione cache
	repository_service.download(video_s3_key, local_video_path)
	
	# Calcolo parametri ottimali
	actual_width = frame_extractor.calculate_target_width(...)
	target_fps = frame_extractor.calculate_extraction_fps(...)
	
	# Estrazione frame con Sharp-Frames
	subprocess.run(["sharp-frames", ...])
	
	# Upload risultati
	phase_zip_helper.create_phase_zip_and_upload(...)
\end{lstlisting}

\subsubsection{Calcolo Parametri Ottimali}

\textbf{Target Width Calculation}

La funzione \texttt{calculate\_target\_width()} risolve il problema dell'adattamento dell'orientamento video ai parametri di qualitÃ  configurati. Nonostante la sua apparente semplicitÃ , implementa una logica fondamentale per garantire coerenza nelle dimensioni di output indipendentemente dall'orientamento del video sorgente.

\begin{lstlisting}[language=python, caption=Logica di adattamento orientamento]
	def calculate_target_width(self, video_path, target_width, target_height):
	# Analisi orientamento video originale
	cap = cv2.VideoCapture(video_path)
	original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
	original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
	is_portrait = original_height > original_width
	
	# Adattamento parametri: per video portrait, 
	# target_height diventa la dimensione principale (width effettiva)
	final_width = target_height if is_portrait else target_width
	return final_width
\end{lstlisting}

\textbf{Problema risolto}: I parametri di qualitÃ  definiscono sempre \texttt{target\_width} e \texttt{target\_height} assumendo orientamento landscape standard. Tuttavia, per video in formato portrait (altezza > larghezza), Ã¨ necessario invertire la logica per mantenere proporzioni corrette e qualitÃ  consistente.

\textbf{Strategia implementata}:
\begin{itemize}
	\item \textbf{Video Landscape}: Utilizza direttamente \texttt{target\_width}
	\item \textbf{Video Portrait}: Utilizza \texttt{target\_height} come dimensione principale
\end{itemize}

Questa logica garantisce che la dimensione principale del frame estratto rispetti sempre i parametri di qualitÃ  configurati, indipendentemente dall'orientamento del video sorgente.

\textbf{Target FPS Calculation}

La funzione \texttt{calculate\_extraction\_fps()} implementa un algoritmo di campionamento temporale adattivo che calcola l'FPS di estrazione ottimale per ottenere il numero target di frame richiesto dal livello di qualitÃ , distribuendoli uniformemente lungo tutta la durata del video.

\begin{lstlisting}[language=python, caption=Algoritmo di campionamento temporale adattivo]
	def calculate_extraction_fps(self, video_path, target_frame_count=180):
	# Analisi caratteristiche video sorgente
	cap = cv2.VideoCapture(video_path)
	video_fps = cap.get(cv2.CAP_PROP_FPS)
	total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
	duration = total_frames / video_fps
	
	# Calcolo FPS di estrazione per distribuzione temporale uniforme
	extraction_fps = target_frame_count / duration
	
	# Applicazione vincoli tecnici
	extraction_fps = min(extraction_fps, video_fps)    # Non puÃ² superare FPS originale
	extraction_fps = max(0.5, extraction_fps)          # Soglia minima anti-sottocampionamento
	
	return round(extraction_fps)
\end{lstlisting}

\textbf{Strategia implementata}:
\begin{enumerate}
	\item \textbf{Copertura temporale completa}: La distribuzione dei frame estratti copre uniformemente l'intera durata del video, evitando concentrazioni in specifici segmenti temporali
	\item \textbf{Adattamento alla durata}: Video piÃ¹ lunghi utilizzeranno FPS di estrazione piÃ¹ bassi per mantenere il target frame count
	\item \textbf{Rispetto vincoli hardware}: L'FPS di estrazione non puÃ² mai superare l'FPS nativo del video sorgente
	\item \textbf{Soglia anti-degrado}: Il limite minimo di 0.5 FPS previene sottocampionamenti eccessivi che comprometterebbero la ricostruzione 3D
\end{enumerate}

\textbf{Esempi di comportamento}:
\begin{itemize}
	\item \textbf{Video 30s a 30fps + target 180 frames}: extraction\_fps = 180/30 = 6 FPS
	\item \textbf{Video 120s a 24fps + target 180 frames}: extraction\_fps = 180/120 = 1.5 FPS  
	\item \textbf{Video 10s a 60fps + target 180 frames}: extraction\_fps = min(18, 60) = 18 FPS
\end{itemize}

\subsubsection{Estrazione Frame con Sharp-Frames}
L'estrazione dei frame utilizza la libreria Sharp-Frames, un tool specializzato nella selezione intelligente di frame basata su metriche di sharpness (Laplacian variance) ottimizzate per applicazioni di ricostruzione 3D e Gaussian Splatting.

\begin{lstlisting}[language=python, caption=Configurazione Sharp-Frames attuale]
	cmd = [
	"sharp-frames",
	local_video_path,           # Video sorgente
	temp_sharp_output,          # Directory output temporanea
	"--selection-method", "best-n",    # Metodo di selezione frame
	"--min-buffer", "3",               # Buffer minimo tra frame consecutivi
	"--fps", str(target_fps)           # FPS di estrazione calcolato
	]
\end{lstlisting}

\textbf{Analisi dei metodi di selezione disponibili}:

\textbf{best-n (configurazione attuale)}: Implementa un approccio a due passate che seleziona un numero target di frame puntando ai piÃ¹ nitidi, mantenendo una distanza minima tra selezioni consecutive. Tuttavia, questo metodo puÃ² causare clustering temporale, concentrando le selezioni in segmenti video di alta qualitÃ  e lasciando gap nella copertura temporale.

\textbf{batched}: Divide il video in batch di dimensione fissa e seleziona il frame piÃ¹ nitido da ogni batch, garantendo copertura temporale uniforme su tutto il materiale sorgente.

\textbf{outlier-removal}: Analizza ogni frame relativamente ai vicini in una finestra scorrevole e rimuove outlier di bassa qualitÃ , mantenendo la distribuzione temporale naturale.

\textbf{Ottimizzazione per COLMAP Sequential}

Per pipeline che utilizzano COLMAP con metodo sequential (anzichÃ© exhaustive), la continuitÃ  temporale Ã¨ prioritaria rispetto alla qualitÃ  assoluta dei singoli frame. Il metodo \texttt{batched} rappresenta la scelta ottimale per questo scenario:

\begin{lstlisting}[language=python, caption=Configurazione ottimizzata per COLMAP Sequential]
	cmd = [
	"sharp-frames",
	local_video_path,
	temp_sharp_output,
	"--selection-method", "batched",    # Copertura temporale garantita
	"--batch-size", "5",               # Batch piccoli per continuitÃ 
	"--batch-buffer", "1",             # Gap minimale tra batch
	"--fps", str(target_fps)
	]
\end{lstlisting}

\textbf{Motivazione parametri batched}:
\begin{itemize}
	\item \textbf{batch-size "5"}: Batch di piccole dimensioni garantiscono granularitÃ  temporale fine, evitando che segmenti video di bassa qualitÃ  vengano completamente esclusi. Con 5 frame per batch, anche sequenze problematiche contribuiscono con almeno un frame alla ricostruzione.
	\item \textbf{batch-buffer "1"}: Gap minimale tra batch consecutivi massimizza la sovrapposizione spaziale tra frame adiacenti, fondamentale per il feature tracking sequenziale di COLMAP. Un buffer maggiore comprometterebbe la continuitÃ  della sequenza.
	\item \textbf{Risultato}: Ogni segmento temporale del video contribuisce uniformemente, mantenendo la densitÃ  di informazione spaziale necessaria per una ricostruzione 3D robusta.
\end{itemize}

\textbf{Vantaggi del metodo batched per Gaussian Splatting}:
\begin{enumerate}
	\item \textbf{Copertura spaziale completa}: Evita zone "cieche" nella ricostruzione dovute a gap temporali
	\item \textbf{Feature tracking robusto}: La continuitÃ  sequenziale facilita il matching tra frame consecutivi
	\item \textbf{Efficienza computazionale}: COLMAP sequential beneficia di sequenze ordinate e continue
	\item \textbf{QualitÃ  bilanciata}: Ogni segmento temporale contribuisce con il suo miglior frame disponibile
\end{enumerate}

\subsubsection{Gestione Output e Archiviazione}
Al termine dell'estrazione, i frame selezionati vengono organizzati nella directory \texttt{input/} e impacchettati nell'archivio \texttt{phase\_frame\_extraction.zip} secondo il pattern di gestione dati intermedi del sistema. Questo approccio abilita le funzionalitÃ  di fork e retry, permettendo di riutilizzare il lavoro di preprocessing per nuovi modelli con parametri di training diversi.

\begin{lstlisting}[language=python, caption=Gestione output e metadati]
	# Upload cartella input (frames) su S3
	is_zip_uploaded = phase_zip_helper.create_phase_zip_and_upload(
	model_id, 
	model_dir, 
	POINT_CLOUD_BUILDING_PHASE_ZIP_NAME, 
	['input']
	)
	
	# Generazione metadati per tracciabilitÃ 
	phase_metadata = {
		"frame_count": len(extracted_frames),
		"video_size_mb": video_size / 1024 / 1024,
		"processing_params": {
			"fps": target_fps,
			"width": actual_width
		}
	}
\end{lstlisting}

Il preprocessing video rappresenta quindi un componente critico della pipeline che, attraverso algoritmi di selezione intelligente e parametrizzazione adattiva, garantisce la preparazione di dataset ottimali per le successive fasi di ricostruzione 3D.



\section{Point Cloud Reconstruction Service}

Il Point Cloud Reconstruction Service rappresenta il componente responsabile della trasformazione dei frame video in una rappresentazione geometrica 3D utilizzabile per il training. Questo servizio implementa tecniche di Structure from Motion (SfM) attraverso una versione ottimizzata di COLMAP, fornendo controllo granulare sui parametri di ricostruzione per bilanciare qualitÃ  e performance.

\subsection{Architettura Containerizzata e Dipendenze}

\subsubsection{Infrastruttura di Base}

Il servizio Ã¨ implementato come container specializzato basato su PyTorch con supporto CUDA completo:

\begin{lstlisting}[language=docker, caption=Dockerfile per COLMAP service con dipendenze ottimizzate]
	FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
	
	ARG TORCH_CUDA_ARCH_LIST="3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX"
	ENV DEBIAN_FRONTEND=noninteractive
	ENV LD_LIBRARY_PATH="/usr/local/lib"
	
	# Installazione dipendenze COLMAP e computer vision
	RUN apt update && apt install -y git libglew-dev libassimp-dev libboost-all-dev \
	cmake ninja-build build-essential libboost-program-options-dev \
	libeigen3-dev libflann-dev libfreeimage-dev libmetis-dev \
	libgoogle-glog-dev libceres-dev libgl1-mesa-glx && \
	apt clean
\end{lstlisting}

\subsubsection{Integrazione COLMAP e Dipendenze Specializzate}

Il container integra COLMAP attraverso Conda per garantire compatibilitÃ  e stabilitÃ :

\begin{lstlisting}[language=docker, caption=Installazione COLMAP e script ottimizzato]
	WORKDIR /workspace/gaussian-splatting
	RUN conda update -n base conda && \
	conda install conda-forge::colmap
	
	# Integrazione script ottimizzato custom
	COPY convert_optimized.py /workspace/convert_optimized.py
	RUN cp /workspace/convert_optimized.py /workspace/gaussian-splatting/convert.py && \
	chmod +x /workspace/gaussian-splatting/convert.py
\end{lstlisting}

\subsubsection{Configurazione Docker Compose}

Il deployment del servizio utilizza configurazioni specifiche per l'accelerazione GPU e l'integrazione con il sistema di storage condiviso:

\begin{lstlisting}[language=docker-compose, caption=Configurazione Docker Compose per COLMAP service]
	colmap-converter-api:
		build:
		 	context: ./colmap-converter
		 	dockerfile: Dockerfile
		container_name: colmap-converter-api
		ports:
			- "8060:8060"
		volumes:
			- shared_data:${MODEL_WORKING_DIR}
			- /tmp/.X11-unix:/tmp/.X11-unix
		environment:
			- XDG_RUNTIME_DIR=/tmp/runtime-root
			- DISPLAY=${DISPLAY}
		deploy:
		resources:
		  reservations:
			devices:
			  - driver: nvidia
				count: 1
				capabilities: [gpu]
\end{lstlisting}

\subsection{Interfaccia API REST}

\subsubsection{Standardizzazione degli Endpoint}

Il servizio espone un'interfaccia REST unificata per l'integrazione con il workflow di elaborazione:

\begin{lstlisting}[language=python, caption=Definizione API REST per COLMAP service]
	from fastapi import FastAPI, HTTPException
	from pydantic import BaseModel
	import subprocess
	import threading
	import queue
	import logging
	
	app = FastAPI()
	
	class ConvertRequest(BaseModel):
		input_dir: str
	
	@app.post("/convert")
	async def run_convert(request: ConvertRequest):
		command = f"python3 /workspace/gaussian-splatting/convert.py -s {request.input_dir}"
		logger.info(f"Starting conversion for directory: {request.input_dir}")
		
		# Esecuzione con logging real-time
		process = subprocess.Popen(
		command,
		shell=True,
		stdout=subprocess.PIPE,
		stderr=subprocess.PIPE,
		text=True,
		bufsize=1,
		)
\end{lstlisting}

\subsubsection{Gestione Asincrona Multi-Thread}

Il sistema implementa lo stesso pattern di logging real-time utilizzato nei training service:

\begin{lstlisting}[language=python, caption=Gestione asincrona output processo COLMAP]
	def stream_output(pipe, q):
		"""Lettura asincrona dell'output del 	processo"""
		try:
			for line in iter(pipe.readline, ''):
				if line:
					q.put(line.strip())
			finally:
				pipe.close()
	
	def log_from_queue(q, process):
		"""Logging continuo fino al completamento del processo"""
		while process.poll() is None or not q.empty():
			try:
				line = q.get_nowait()
				logger.info(line)
			except queue.Empty:
				continue
\end{lstlisting}

\subsection{Ottimizzazioni COLMAP Custom}

\subsubsection{Script di Conversione Ottimizzato}

Il servizio utilizza una versione customizzata del script \texttt{convert.py} originale del progetto Gaussian Splatting, estesa con parametri di ottimizzazione aggiuntivi per migliorare le performance su input video sequenziali:

\begin{lstlisting}[language=python, caption=Parametri ottimizzazione custom per COLMAP]
	parser = ArgumentParser("Colmap converter")
	parser.add_argument("--source_path", "-s", required=True, type=str)
	parser.add_argument("--camera", default="OPENCV", type=str)
	parser.add_argument("--no_gpu", action='store_true')
	parser.add_argument("--skip_matching", action='store_true')
	
	# Parametri di ottimizzazione custom
	parser.add_argument("--max_features", default=8000, type=int, 
	help="Maximum number of features per image")
	parser.add_argument("--matching_strategy", default="sequential", 
	choices=["exhaustive", "sequential", "vocab_tree"],
	help="Feature matching strategy")
	parser.add_argument("--overlap", default=10, type=int, 
	help="Number of overlapping images for sequential matching")
\end{lstlisting}

\textbf{Parametri Aggiunti}:
\begin{itemize}
	\item \texttt{--max\_features=8000}: Limita il numero massimo di feature SIFT estratte per immagine, bilanciando qualitÃ  e performance
	\item \texttt{--matching\_strategy}: Seleziona la strategia di matching ottimale per il tipo di contenuto
	\item \texttt{--overlap=10}: Controlla il numero di immagini successive nel sequential matching
\end{itemize}

\subsubsection{Strategie di Feature Matching Adaptive}

Il sistema implementa tre strategie di matching configurabili, ciascuna ottimizzata per diversi tipi di contenuto e vincoli computazionali:

\begin{lstlisting}[language=python, caption=Implementazione strategie matching adaptive]
	if args.matching_strategy == "exhaustive":
	# Matching exhaustive - massima qualitÃ , tempo elevato
	feat_matching_cmd = colmap_command + " exhaustive_matcher \
	--database_path " + args.source_path + "/distorted/database.db \
	--SiftMatching.use_gpu " + str(use_gpu)
	
	elif args.matching_strategy == "sequential":
	# Sequential matching - ottimizzato per sequenze video
	feat_matching_cmd = colmap_command + " sequential_matcher \
	--database_path " + args.source_path + "/distorted/database.db \
	--SiftMatching.use_gpu " + str(use_gpu) + " \
	--SequentialMatching.overlap " + str(args.overlap) + " \
	--SequentialMatching.quadratic_overlap 1"
	
	elif args.matching_strategy == "vocab_tree":
	# Vocabulary tree matching - veloce per dataset di grandi dimensioni
	feat_matching_cmd = colmap_command + " vocab_tree_matcher \
	--database_path " + args.source_path + "/distorted/database.db \
	--SiftMatching.use_gpu " + str(use_gpu)
\end{lstlisting}

\paragraph{Exhaustive Matching (Originale)}
La strategia \texttt{exhaustive} rappresenta l'approccio originale del sistema, dove ogni immagine viene confrontata con tutte le altre nel dataset. Questa strategia garantisce la massima qualitÃ  di ricostruzione catturando tutti i possibili match tra le immagini, ma presenta complessitÃ  computazionale O(nÂ²) che la rende impraticabile per dataset di grandi dimensioni. Per 200 frame video, richiede circa 2-4 ore di elaborazione, rendendola inadatta per applicazioni interattive.

\paragraph{Sequential Matching (Ottimizzato)}
La strategia \texttt{sequential} sfrutta l'ordine temporale naturale dei frame video, confrontando ogni immagine solo con le successive nell'ordine di acquisizione. Con \texttt{overlap=10}, ogni frame viene confrontato con i 10 frame successivi, riducendo la complessitÃ  computazionale a O(n Ã— overlap). Questa strategia Ã¨ particolarmente efficace per contenuti video dove la continuitÃ  temporale garantisce sovrapposizione sufficiente tra frame adiacenti. Mantiene circa il 95\% della qualitÃ  di ricostruzione dell'exhaustive matching riducendo i tempi del 75-80\% (15-30 minuti per 200 frame).

\paragraph{Vocabulary Tree Matching (Scalabile)}
La strategia \texttt{vocab\_tree} utilizza un vocabulary tree precompilato per identificare rapidamente coppie di immagini con alta probabilitÃ  di match, riducendo ulteriormente la complessitÃ  a O(n log n). Questa strategia Ã¨ ottimale per dataset molto grandi (>1000 immagini) ma richiede la disponibilitÃ  di un vocabulary tree appropriato e puÃ² produrre risultati meno accurati in scene con poche feature distintive.
\newline
\noindent Il sistema adotta \texttt{sequential} come strategia default poichÃ© rappresenta il compromesso ottimale per il caso d'uso principale (elaborazione immagini estratte sequenzialmente da un video). A differenza dell'exhaustive matching originale, che garantisce qualitÃ  massima ma tempi proibitivi, il sequential matching sfrutta la struttura temporale intrinseca dei video per mantenere alta qualitÃ  con performance acceptable per applicazioni interattive. Il vocabulary tree matching, pur essendo piÃ¹ veloce, non Ã¨ adatto a video di durata tipica e richiede preprocessing aggiuntivo.

\subsubsection{Ottimizzazioni Feature Extraction}

Il processo di estrazione delle caratteristiche Ã¨ stato ottimizzato per bilanciare velocitÃ  e qualitÃ :

\begin{lstlisting}[language=python, caption=Feature extraction ottimizzata SIFT]
	feat_extracton_cmd = colmap_command + " feature_extractor \
	--database_path " + args.source_path + "/distorted/database.db \
	--image_path " + args.source_path + "/input \
	--ImageReader.single_camera 1 \
	--ImageReader.camera_model " + args.camera + " \
	--SiftExtraction.use_gpu " + str(use_gpu) + " \
	--SiftExtraction.max_num_features " + str(args.max_features) + " \
	--SiftExtraction.first_octave -1 \
	--SiftExtraction.num_octaves 4 \
	--SiftExtraction.octave_resolution 3"
\end{lstlisting}

\subsection{Pipeline di Ricostruzione Completa}

\subsubsection{Fasi del Processo SfM}

La pipeline implementa le fasi standard di Structure from Motion con parametri ottimizzati:

\begin{enumerate}
	\item \textbf{Feature Extraction}: Estrazione SIFT features con controllo del numero massimo per immagine
	\item \textbf{Feature Matching}: Strategia adaptive basata sul tipo di contenuto
	\item \textbf{Bundle Adjustment}: Ottimizzazione geometrica con tolleranze personalizzate
	\item \textbf{Image Undistortion}: Correzione distorsioni per ottenere intrinseci pinhole ideali
\end{enumerate}

\subsubsection{Bundle Adjustment Ottimizzato}

Il processo di bundle adjustment utilizza parametri ottimizzati per ridurre i tempi di computazione:

\begin{lstlisting}[language=python, caption=Bundle adjustment con parametri ottimizzati]
	mapper_cmd = (colmap_command + " mapper \
	--database_path " + args.source_path + "/distorted/database.db \
	--image_path "  + args.source_path + "/input \
	--output_path "  + args.source_path + "/distorted/sparse \
	--Mapper.ba_global_function_tolerance=0.000001 \
	--Mapper.ba_global_max_num_iterations=50 \
	--Mapper.ba_local_max_num_iterations=10 \
	--Mapper.tri_ignore_two_view_tracks=1 \
	--Mapper.min_num_matches=15")
\end{lstlisting}

\textbf{Parametri di Ottimizzazione}:
\begin{itemize}
	\item \texttt{ba\_global\_function\_tolerance=0.000001}: Tolleranza piÃ¹ stretta per maggiore precisione
	\item \texttt{ba\_global\_max\_num\_iterations=50}: Limite iterazioni per evitare convergenza lenta
	\item \texttt{ba\_local\_max\_num\_iterations=10}: Ottimizzazione locale piÃ¹ veloce
	\item \texttt{tri\_ignore\_two\_view\_tracks=1}: Ignora track con solo 2 viste (meno affidabili)
	\item \texttt{min\_num\_matches=15}: Soglia minima per considerare affidabile una coppia di immagini
\end{itemize}

\subsubsection{Gestione Output e Struttura Dati}

Il servizio genera una struttura dati standardizzata compatibile con tutti gli algoritmi di training:

\begin{itemize}
	\item \textbf{Database.db}: Database COLMAP con features e matches
	\item \textbf{Sparse/0/}: Ricostruzione geometrica (cameras.bin, images.bin, points3D.bin)
	\item \textbf{Images/}: Immagini undistorted per il training
\end{itemize}

\textbf{Performance Impact}: Le ottimizzazioni implementate riducono significativamente i tempi di elaborazione per contenuti video tipici, da 2-4 ore per 200 frame con matching exhaustive a 15-30 minuti con sequential matching, mantenendo circa il 95\% della qualitÃ  di ricostruzione originale.

\subsection{Integrazione con il Workflow}

\subsubsection{Coordinamento con Job Executor}

Il servizio si integra perfettamente nel flusso sequenziale gestito dal job executor, ricevendo come input la directory contenente i frame estratti e producendo la struttura dati necessaria per il training:

\begin{verbatim}
	Input:  /model_dir/input/     (frame estratti da video)
	Output: /model_dir/sparse/    (ricostruzione geometrica)
	/model_dir/images/    (immagini undistorted)
\end{verbatim}

\subsubsection{Gestione Errori e Logging}

Il sistema implementa gestione robusta degli errori con logging dettagliato per ogni fase del processo COLMAP, facilitando il debugging e il monitoraggio delle prestazioni in ambiente di produzione.

\section{Motore di Training}

Il motore di training rappresenta il cuore computazionale del sistema, responsabile dell'esecuzione degli algoritmi di Gaussian Splatting. L'implementazione adotta un approccio containerizzato che isola ogni algoritmo in un ambiente dedicato, esponendo le funzionalitÃ  attraverso API REST standardizzate.

\subsection{Architettura Containerizzata Multi-Algoritmo}

\subsubsection{Strategia di Containerizzazione}

Il sistema implementa quattro container specializzati, ciascuno dedicato a un algoritmo specifico:

\begin{itemize}
	\item \textbf{gaussian-splatting}: Implementazione di riferimento del paper originale
	\item \textbf{3dgs-mcmc}: Variante con campionamento MCMC per ottimizzazione stocastica  
	\item \textbf{taming-3dgs}: Versione ottimizzata per scene complesse
\end{itemize}

Ogni container segue una strategia di build multi-stage per ottimizzare le dimensioni e i tempi di compilazione:

\begin{lstlisting}[language=docker, caption=Dockerfile.base - Stage di installazione]
	FROM nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04
	
	ENV DEBIAN_FRONTEND=noninteractive
	
	# Installazione dipendenze CUDA e build tools
	RUN apt update && apt upgrade -y && apt install -y \
	git cmake libxmu-dev libxi-dev libgl-dev libomp-dev \
	python3-dev python3-venv python3-pip \
	build-essential ninja-build wget \
	libboost-program-options-dev libboost-filesystem-dev \
	gcc-10 g++-10 libatlas-base-dev libsuitesparse-dev
	
	# Clone e setup del repository specifico
	WORKDIR /workspace
	RUN git clone https://github.com/graphdeco-inria/gaussian-splatting --recursive
	
	# Installazione PyTorch con supporto CUDA
	RUN pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu121
	
	# Compilazione moduli CUDA nativi
	RUN cd submodules/diff-gaussian-rasterization && pip3 install .
	RUN cd submodules/simple-knn && pip3 install .
	RUN cd submodules/fused-ssim && pip3 install .
\end{lstlisting}

\begin{lstlisting}[language=docker, caption=Dockerfile con porta configurabile tramite variabile di ambiente]
	FROM gaussian-splatting
	
	# Porta configurabile tramite variabile di ambiente
	ARG API_PORT=8100
	ENV API_PORT=$API_PORT
	EXPOSE $API_PORT
	
	WORKDIR /workspace/gaussian-splatting
	COPY api.py /workspace/gaussian-splatting/api.py
	RUN pip3 install fastapi uvicorn
	
	ENV QT_QPA_PLATFORM=offscreen
	ENV PYTHONUNBUFFERED=1
	
	CMD ["sh", "-c", "uvicorn api:app --host 0.0.0.0 --port $API_PORT"]
\end{lstlisting}

\subsubsection{Vantaggi dell'Approccio Multi-Stage}

La separazione in due fasi di build offre diversi vantaggi:

\begin{itemize}
	\item \textbf{Riutilizzo delle dipendenze}: Il base container puÃ² essere condiviso tra diverse versioni
	\item \textbf{Tempi di build ottimizzati}: Solo la fase API viene ricompilata per modifiche minori
	\item \textbf{Isolamento delle responsabilitÃ }: Separazione tra setup dell'algoritmo e esposizione servizi
\end{itemize}

La configurazione Docker Compose coordina il deployment dei container specificando porte e parametri per ciascun algoritmo:

\begin{lstlisting}[language=docker-compose, caption=Configurazione Docker Compose con porte dinamiche]
	gaussian-splatting-api:
	build:
	context: ./gaussian-splatting  # Nuovo servizio API per Gaussian Splatting
	dockerfile: Dockerfile
	args:
	- API_PORT=${GAUSSIAN_SPLATTING_PORT:-8100}
	container_name: gaussian-splatting-api
	volumes:
	- shared_data:${MODEL_WORKING_DIR}   # Stesso volume condiviso
	- /tmp/.X11-unix:/tmp/.X11-unix  # Monta il socket X11
	ports:
	- "${GAUSSIAN_SPLATTING_PORT:-8100}:${GAUSSIAN_SPLATTING_PORT:-8100}"
	depends_on:
	- gaussian-splatting
	environment:
	- API_PORT=${GAUSSIAN_SPLATTING_PORT:-8100}
	- MODEL_WORKING_DIR=${MODEL_WORKING_DIR}
	- XDG_RUNTIME_DIR=/tmp/runtime-root
	- DISPLAY=${DISPLAY}  # Usa il DISPLAY del sistema host
	networks:
	- my_network
	deploy:
	resources:
	reservations:
	devices:
	- driver: nvidia
	count: 1  # Usa tutte le GPU disponibili
	capabilities: [gpu]
	
	3dgs-mcmc-api:
	build:
	context: ./3dgs-mcmc  # Nuovo servizio API per Gaussian Splatting
	dockerfile: Dockerfile
	args:
	- API_PORT=${MCMC_PORT:-8100}
	container_name: 3dgs-mcmc-api
	volumes:
	- shared_data:${MODEL_WORKING_DIR}   # Stesso volume condiviso
	- /tmp/.X11-unix:/tmp/.X11-unix  # Monta il socket X11
	ports:
	- "${MCMC_PORT:-8100}:${MCMC_PORT:-8100}"
	depends_on:
	- 3dgs-mcmc
	mem_limit: 20G  # Imposta un limite di memoria piÃ¹ alto (ad esempio 32 GB)
	memswap_limit: 24G  # Limite di memoria + swap a 36 GB
	environment:
	- API_PORT=${MCMC_PORT:-8101}
	- MODEL_WORKING_DIR=/app/shared_data
	- XDG_RUNTIME_DIR=/tmp/runtime-root
	- DISPLAY=${DISPLAY}  # Usa il DISPLAY del sistema host
	networks:
	- my_network
	deploy:
	resources:
	reservations:
	devices:
	- driver: nvidia
	count: 1  # Usa tutte le GPU disponibili
	capabilities: [gpu]
	
	taming-3dgs-api:
	build:
	context: ./taming-3dgs  # Nuovo servizio API per Gaussian Splatting
	dockerfile: Dockerfile
	args:
	- API_PORT=${TAMING_PORT:-8100}
	container_name: taming-3dgs-api
	
	mem_limit: 20G  # Imposta un limite di memoria piÃ¹ alto (ad esempio 32 GB)
	memswap_limit: 24G  # Limite di memoria + swap a 36 GB
	volumes:
	- shared_data:${MODEL_WORKING_DIR}   # Stesso volume condiviso
	- /tmp/.X11-unix:/tmp/.X11-unix  # Monta il socket X11
	ports:
	- "${TAMING_PORT:-8100}:${TAMING_PORT:-8100}"
	depends_on:
	- taming-3dgs
	environment:
	- API_PORT=${TAMING_PORT:-8102}
	- MODEL_WORKING_DIR=/app/shared_data
	- XDG_RUNTIME_DIR=/tmp/runtime-root
	- DISPLAY=${DISPLAY}  # Usa il DISPLAY del sistema host
	networks:
	- my_network
	deploy:
	resources:
	reservations:
	devices:
	- driver: nvidia
	count: 1  # Usa tutte le GPU disponibili
	capabilities: [gpu]
\end{lstlisting}

La configurazione utilizza variabili di ambiente definite nel file \texttt{.env} per specificare le porte di ciascun container:

\begin{lstlisting}[language=bash, caption=File .env con configurazione porte]
	GAUSSIAN_SPLATTING_PORT=8100
	MCMC_PORT=8101
	TAMING_PORT=8102
	NERFSTUDIO_GSPLAT_PORT=8103
\end{lstlisting}

Questo approccio consente di modificare facilmente le porte di esposizione modificando solo il file di configurazione, senza dover ricompilare i container o modificare i Dockerfile individuali.

\subsection{Interfaccia API REST Unificata}

\subsubsection{Standardizzazione degli Endpoint}

Ogni container espone un'interfaccia REST standardizzata che astrae le specificitÃ  implementative degli algoritmi sottostanti:

\begin{lstlisting}[language=python, caption=Definizione modelli dati per API REST]
	from fastapi import FastAPI, HTTPException
	from pydantic import BaseModel, Field
	import subprocess
	import threading
	import queue
	import logging
	from typing import Dict, Any
	
	app = FastAPI()
	
	class TrainRequest(BaseModel):
	input_dir: str  
	output_dir: str
	params: Dict[str, Any] = Field(
	default_factory=dict, 
	description="Parametri specifici dell'algoritmo"
	)
	
	class RenderRequest(BaseModel):
	output_dir: str  
	
	class MetricsRequest(BaseModel):
	output_dir: str
\end{lstlisting}

\subsubsection{Gestione Parametri Dinamica}

La gestione dei parametri di training Ã¨ implementata attraverso un sistema dinamico che costruisce automaticamente gli argomenti della command line:

\begin{lstlisting}[language=python, caption=Costruzione dinamica comando di training]
	@app.post("/train")
	async def run_train(request: TrainRequest):
	logger.info(f"Starting training - Input: {request.input_dir}")
	
	# Costruzione dinamica del comando
	command = ["python3", "/workspace/gaussian-splatting/train.py"]
	
	# Parametri obbligatori
	command.extend(["-s", request.input_dir, "-m", request.output_dir])
	
	# Parametri dinamici dall'utente
	boolean_flags = {"eval"}
	
	for param_key, value in request.params.items():
	if param_key in boolean_flags:
	if value:
	command.append(f"--{param_key}")
	else:
	command.extend([f"--{param_key}", str(value)])
	
	# Esecuzione con logging real-time
	process = subprocess.Popen(
	command,
	stdout=subprocess.PIPE,
	stderr=subprocess.PIPE,
	text=True,
	bufsize=1,
	)
\end{lstlisting}

\subsection{Gestione Asincrona e Logging Real-Time}

\subsubsection{Sistema di Logging Multithreaded}

Per gestire processi di training che possono durare ore, Ã¨ stato implementato un sistema di logging real-time basato su thread dedicati:

\begin{lstlisting}[language=python, caption=Funzioni di gestione output asincrono]
	def stream_output(pipe, q):
	"""Lettura asincrona dell'output del processo"""
	try:
	for line in iter(pipe.readline, ''):
	if line:
	q.put(line.strip())
	finally:
	pipe.close()
	
	def log_from_queue(q, process):
	"""Logging continuo fino al completamento del processo"""
	while process.poll() is None or not q.empty():
	try:
	line = q.get_nowait()
	logger.info(line)
	except queue.Empty:
	continue
\end{lstlisting}

\subsubsection{Orchestrazione Multi-Thread}

Il sistema utilizza quattro thread dedicati per gestire l'I/O del processo di training:

\begin{lstlisting}[language=python, caption=Gestione multi-thread per I/O processo]
	# Creazione code per stdout e stderr
	stdout_queue = queue.Queue()
	stderr_queue = queue.Queue()
	
	# Thread per lettura output
	stdout_thread = threading.Thread(target=stream_output, args=(process.stdout, stdout_queue))
	stderr_thread = threading.Thread(target=stream_output, args=(process.stderr, stderr_queue))
	
	# Thread per logging
	stdout_log_thread = threading.Thread(target=log_from_queue, args=(stdout_queue, process))
	stderr_log_thread = threading.Thread(target=log_from_queue, args=(stderr_queue, process))
	
	# Avvio coordinato di tutti i thread
	for thread in [stdout_thread, stderr_thread, stdout_log_thread, stderr_log_thread]:
	thread.start()
\end{lstlisting}

\subsection{Pipeline di Elaborazione Completa}

\subsubsection{Endpoint Specializzati}

Oltre al training principale, ogni container espone endpoint per le fasi complementari del workflow:

\begin{itemize}
	\item \textbf{\texttt{/train}}: Esecuzione dell'algoritmo di training principale
	\item \textbf{\texttt{/render}}: Generazione delle immagini di test per validazione
	\item \textbf{\texttt{/metrics}}: Calcolo delle metriche di qualitÃ  (PSNR, SSIM, LPIPS)
\end{itemize}

\subsubsection{Gestione Errori e Resilienza}

Il sistema implementa meccanismi robusti di gestione degli errori:

\begin{lstlisting}[language=python, caption=Gestione errori e cleanup risorse]
	try:
	process.wait()
	
	# Attesa completamento thread
	for thread in [stdout_thread, stderr_thread, stdout_log_thread, stderr_log_thread]:
	thread.join()
	
	if process.returncode != 0:
	logger.error("Training failed")
	raise HTTPException(status_code=500, detail={"error": "Training failed"})
	
	logger.info("Training completed successfully")
	return {"message": "Training completed successfully"}
	
	except Exception as e:
	logger.error(f"Unexpected error: {str(e)}")
	process.kill()  # Terminazione forzata in caso di errore
	raise HTTPException(status_code=500, detail={"error": "Training failed", "stderr": str(e)})
\end{lstlisting}

\subsection{Ottimizzazioni CUDA e Performance}

\subsubsection{Configurazione Ambiente CUDA}

I container sono configurati per sfruttare le GPU NVIDIA con ottimizzazioni specifiche:

\begin{lstlisting}[language=docker, caption=Configurazione ambiente CUDA]
	# Configurazione architetture CUDA supportate
	ENV TORCH_CUDA_ARCH_LIST="8.6 8.9+PTX"
	ENV PATH=/usr/local/cuda/bin:$PATH
	ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
	
	# Ambiente headless per rendering server-side
	ENV QT_QPA_PLATFORM=offscreen
	ENV PYTHONUNBUFFERED=1
\end{lstlisting}

\subsubsection{Compilazione Moduli Nativi}

Ogni algoritmo richiede la compilazione di moduli CUDA specifici per le operazioni di rasterizzazione:

\begin{itemize}
	\item \textbf{diff-gaussian-rasterization}: implementa la rasterizzazione differenziabile delle primitive gaussiane 3D. Questo componente gestisce la conversione delle gaussiane tridimensionali in pixel 2D sull'immagine finale, mantenendo la capacitÃ  di calcolare i gradienti necessari per l'ottimizzazione durante il training. Ãˆ il cuore del sistema di rendering che permette velocitÃ  real-time una volta completato il training.
	\item \textbf{simple-knn}: fornisce algoritmi k-nearest neighbors ottimizzati per GPU, utilizzati per la gestione dinamica delle gaussiane durante il training. Questo componente identifica le gaussiane vicine nello spazio 3D, supportando le operazioni di densificazione adattiva che permettono al sistema di aggiungere o rimuovere gaussiane in base alle necessitÃ  della scena.
	\item \textbf{fused-ssim}: implementa il calcolo accelerato della metrica SSIM (Structural Similarity Index) utilizzata nella loss function durante il training. Questa componente valuta la similaritÃ  strutturale tra l'immagine renderizzata e quella di riferimento, contribuendo all'ottimizzazione della qualitÃ  percettiva del modello finale.
\end{itemize}

La compilazione avviene durante la fase di build del container, garantendo ottimizzazioni specifiche per l'hardware target e riducendo i tempi di startup dei job di training.

\subsection{API Gateway}

\subsubsection{Architettura e Ruolo nel Sistema}
L'API Gateway rappresenta il punto di ingresso unificato per tutte le richieste verso il backend del sistema. Implementato come servizio FastAPI containerizzato, fornisce un'interfaccia REST standardizzata che orchestra le interazioni tra frontend, database MongoDB, sistema di code RabbitMQ e storage S3.

\textbf{Configurazione Container}:
\begin{lstlisting}[language=docker-compose, caption=Configurazione Docker Compose API Gateway]
	api-gateway:
	build:
	context: ./api-gateway
	dockerfile: ${BACKEND_DOCKERFILE_NAME}
	container_name: api-gateway
	ports:
	- "8000:8000"
	depends_on:
	mongo:
	condition: service_healthy
	rabbitmq:
	condition: service_healthy
	environment:
	- AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
	- AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
	- MONGO_URI=mongodb://mongo:27017/3dgs_models
	- RABBITMQ_HOSTNAME=rabbitmq
	- RABBITMQ_USER=${RABBIT_MQ_USER_WRITER}
\end{lstlisting}

\textbf{Architettura Multi-Stage Docker}:
Il container utilizza un approccio multi-stage che combina FastAPI con Nginx come reverse proxy, garantendo performance ottimali e gestione del traffico professionale.

\subsubsection{Sistema di Autenticazione}
L'API Gateway implementa un sistema di autenticazione ibrido che combina HTTP Basic Authentication per operazioni amministrative e JWT Bearer tokens per l'accesso utente standard.

\begin{lstlisting}[language=python, caption=Implementazione sistema di autenticazione]
	# HTTP Basic Authentication per operazioni admin
	def verify_basic_auth(credentials: HTTPBasicCredentials = Security(security)):
	valid_username = "admin"
	valid_password = "supersecret"
	
	is_valid_username = secrets.compare_digest(credentials.username, valid_username)
	is_valid_password = secrets.compare_digest(credentials.password, valid_password)
	
	if not (is_valid_username and is_valid_password):
	raise HTTPException(status_code=401, detail="Invalid credentials")
	
	return credentials.username
	
	# JWT Token Authentication per utenti
	@app.post("/token")
	async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):
	user = get_user(form_data.username)
	if not user or not verify_password(form_data.password, user.hashed_password):
	raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)
	
	access_token = create_access_token({"sub": user.username})
	return {"access_token": access_token, "token_type": "bearer"}
\end{lstlisting}

\textbf{Gestione Utenti}:
Il sistema utilizza bcrypt per l'hashing delle password e implementa funzionalitÃ  complete di gestione utenti:

\begin{lstlisting}[language=python, caption=Servizio gestione utenti]
	def create_user(username: str, password: str, role: str = "user"):
	hashed_password = hash_password(password)
	user = UserInDB(username=username, hashed_password=hashed_password, role=role)
	result = users_collection.insert_one(user.dict())
	return str(result.inserted_id)
	
	def get_user(username: str):
	user = users_collection.find_one({"username": username})
	if user:
	return UserInDB(**user)
	return None
\end{lstlisting}

\subsubsection{CRUD Operations sui Modelli}
L'API Gateway espone un'interfaccia REST completa per la gestione dei modelli 3D, implementando operazioni di creazione, lettura, aggiornamento e cancellazione con funzionalitÃ  avanzate di paginazione, ordinamento e filtri.

\textbf{Creazione Modelli}:
L'endpoint di creazione supporta tre modalitÃ  operative distinte:

\begin{lstlisting}[language=python, caption=Endpoint creazione modello]
	@app.post("/models/", response_model=ModelResponse, dependencies=[Depends(verify_basic_auth)])
	async def create_model(request: ModelCreateRequest):
	# Creazione modello in MongoDB
	model = await model_service.create_model_in_db(request)
	
	# Invio job a RabbitMQ per avvio workflow
	queue_job_service.send_job(model.id, model.current_phase)
	
	return model
\end{lstlisting}

\textbf{ModalitÃ  di creazione supportate}:
\begin{enumerate}
	\item \textbf{Creazione ex-novo}: Upload di nuovo video tramite presigned URL S3
	\item \textbf{Fork da modello esistente}: Riutilizzo di fasi intermedie completate
	\item \textbf{Retry}: Riavvio di modelli con fasi fallite
\end{enumerate}

\textbf{Implementazione Fork}:
La funzionalitÃ  di fork implementa una logica sofisticata di riutilizzo delle fasi intermedie:

\begin{lstlisting}[language=python, caption=Logica di validazione fork]
	def _validate_fork_prerequisites(self, parent_project: ModelResponse, from_phase_str: str):
	phase_order = [Phase.FRAME_EXTRACTION, Phase.POINT_CLOUD_BUILDING, Phase.TRAINING]
	from_phase = Phase(from_phase_str)
	from_phase_idx = phase_order.index(from_phase)
	
	# Verifica completamento fasi prerequisite
	for i in range(from_phase_idx):
	required_phase = phase_order[i]
	if required_phase not in parent_project.phases:
	raise ValueError(f"Parent project missing required phase: {required_phase}")
	
	phase_status = parent_project.phases[required_phase].status
	if phase_status not in [PhaseStatus.COMPLETED, PhaseStatus.FAILED]:
	raise ValueError(f"Parent project must have completed {required_phase}")
\end{lstlisting}

\textbf{Gestione Retry}:
Il sistema di retry identifica automaticamente le fasi interrotte e le reimposta per una nuova esecuzione:

\begin{lstlisting}[language=python, caption=Implementazione retry]
	async def update_model_for_retry(self, model_id: UUID) -> ModelResponse:
	model = self.get_model_by_id(UUID(model_id))
	
	# Identifica fase interrotta
	interrupted_phase = None
	for phase_name, phase_data in model.phases.items():
	if phase_data.status in [PhaseStatus.FAILED, PhaseStatus.RUNNING, PhaseStatus.PENDING]:
	interrupted_phase = phase_name
	break
	
	# Reset fase per nuovo tentativo
	update_data = {
		"overall_status": "RUNNING",
		"current_phase": interrupted_phase,
		f"phases.{interrupted_phase}.status": "PENDING",
		f"phases.{interrupted_phase}.started_at": None,
		f"phases.{interrupted_phase}.error_message": None
	}
	
	self.db["models"].update_one({"_id": str(model_id)}, {"$set": update_data})
	return self.get_model_by_id(UUID(model_id))
\end{lstlisting}

\textbf{Listing con Paginazione Avanzata}:
L'endpoint di listing implementa funzionalitÃ  enterprise-grade di ricerca e navigazione:

\begin{lstlisting}[language=python, caption=Endpoint listing modelli]
	@app.get("/models/", response_model=PaginatedModelResponse)
	async def list_models(
	page: int = Query(1, alias="page", ge=1),
	limit: int = Query(10, alias="limit", ge=1, le=100),
	sort_by: Optional[str] = Query(None, regex="^(model_name|status|created_at)$"),
	order: Optional[str] = Query("asc", regex="^(asc|desc)$"),
	title: Optional[str] = Query(None),
	status: Optional[List[str]] = Query(None)
	):
	models, total_count = model_service.list_models_from_db(
	page, limit, sort_by, order, title_filter=title, status_filter=status
	)
	
	total_pages = (total_count + limit - 1) // limit
	
	return PaginatedModelResponse(
	models=models,
	totalCount=total_count,
	totalPages=total_pages,
	page=page
	)
\end{lstlisting}

\subsubsection{Producer per Sistema di Workflow}
L'API Gateway funge da producer primario per il sistema di code RabbitMQ, orchestrando l'avvio e la gestione dei workflow di elaborazione attraverso il QueueJobService.

\begin{lstlisting}[language=python, caption=Integrazione con sistema di code]
	# Invio job per nuovi modelli
	queue_job_service.send_job(model.id, model.current_phase)
	
	# Reinvio job per retry
	queue_job_service.send_job(model.id, model.current_phase)
\end{lstlisting}

\textbf{Gestione Presigned URLs}:
Per ottimizzare l'upload di video di grandi dimensioni, l'API Gateway genera presigned URLs che permettono upload diretti a S3:

\begin{lstlisting}[language=python, caption=Generazione presigned URL]
	@app.post("/s3/upload-url/")
	async def get_upload_url(request: PresignedUrlRequest):
	upload_id = str(uuid4())
	s3_key = f"{S3_VIDEOS_PREFIX}/{upload_id}/{request.filename}"
	
	presigned_url = repository_service.generate_presigned_url_upload(
	s3_key, request.content_type
	)
	
	return {
		"upload_id": upload_id, 
		"upload_url": presigned_url,
		"video_s3_key": s3_key
	}
\end{lstlisting}

\subsubsection{Sistema di Notifiche Real-time}
L'API Gateway implementa un sistema di notifiche WebSocket per aggiornamenti real-time sullo stato dei modelli:

\begin{lstlisting}[language=python, caption=WebSocket notifications]
	@app.websocket("/ws/notifications")
	async def websocket_notifications(websocket: WebSocket):
	await websocket.accept()
	active_connections.add(websocket)
	
	try:
	while True:
	data = await websocket.receive_text()
	if data == "ping":
	await websocket.send_text("pong")
	except Exception as e:
	print(f"WebSocket connection error: {e}")
	finally:
	active_connections.discard(websocket)
	
	async def notify_clients(change_data: dict):
	await process_and_send_notification(change_data, active_connections)
\end{lstlisting}

L'API Gateway rappresenta quindi il cuore dell'architettura del sistema, fornendo un punto di accesso unificato e sicuro che orchestra tutte le operazioni di gestione dei modelli 3D, dall'upload iniziale alla distribuzione dei risultati finali.